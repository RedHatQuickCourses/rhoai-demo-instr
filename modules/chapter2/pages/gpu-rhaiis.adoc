= GPUs for AI Inference

[#introduction]
== Introduction

Successfully deploying a Large Language Model is only half the battle. To build a cost-effective and performant AI platform, you must accurately match the model's requirements to the available hardware. Simply throwing oversized, expensive GPUs at every problem is a wasteful and ineffective strategy that leads to high costs and underutilized resources.

This module equips you with the fundamental knowledge and tools to analyze an LLM's requirements and make informed decisions about the necessary GPU infrastructure. You will learn how to estimate the memory needed for a model and its KV Cache, understand the impact of quantization, and use sizing tools to plan for future deployments.

image::intro_gpu_size.gif[.Objectives, 600]

[#lab-story]
== Lab Story

The Platform Engineering team at "InnovateForward Corp" is getting requests to deploy a variety of new models. The business wants to know: "What hardware do we need to run the new 70-billion-parameter model?" and "Can we run this smaller, optimized model on our existing A10G GPUs to save costs?"

To answer these questions confidently, the team needs to move beyond guesswork. They must learn how to calculate the video RAM (vRAM) footprint of a given model, including its parameters and the KV Cache. Mastering these calculations will enable them to provide accurate hardware recommendations, optimize resource utilization, and justify infrastructure costs to leadership.

[#understanding-vram]
==  The True Cost Blueprint

=== 1. Model Weights (The Baseline Cost)

This is the memory needed to load the model's parameters. It's a function of the model size (billions of parameters) and the numerical precision used to store each parameter.

Calculation:
A formula to estimate the memory for model weights, including overhead:

....
M = ((P * 4) / (32 / Q)) * 1.2
....

[cols="1,4"]
|===
| Symbol | Description
| M | Total GPU memory required for the model weights (in bytes).
| P | The number of parameters in the model (e.g., 8 billion).
| 4 (bytes) | The size of a full-precision parameter in bytes (FP32).
| Q | The number of bits used for the model's data type after quantization (e.g., 16 for FP16, 8 for INT8).
| 1.2| A multiplier representing a ~20% overhead for loading additional components into GPU memory.
|===

For example, let's calculate the requirement for the ibm-granite/granite-3.3-8b-instruct model, which has 8 billion parameters and uses the FP16 data type (16 bits).

[source,bash]
Formula: (((Parameters * 4 bytes) / (32 / Quantization_Bits)) * 1.2)
Calculation: (((8 * 4) / (32 / 16)) * 1.2) / = ~19 Gi


[]
****
*Quantization*

While not directly part of the sizing calculation, Quantization is a technique used to reduce the size and improve the performance of an LLM by converting its weights to a lower-precision data type. While many models default to FP16 (16 bits), it's common to quantize them to INT8 (8 bits) or even INT4 (4 bits) to significantly reduce their memory footprint. *Quantization is the most direct way to reduce the baseline memory cost.*

.Estimated vRAM for Model Weights by Precision
[options="header"]
|===
| Model Size (Parameters) | FP16 / BF16 (16-bit) | INT8 (8-bit) | INT4 (4-bit)
| 1 Billion | ~2 GB | ~1 GB | ~0.5 GB
| 3 Billion | ~6 GB | ~3 GB | ~1.5 GB
| 8 Billion | ~16 GB | ~8 GB | ~4 GB
| 13 Billion| ~26 GB | ~13 GB | ~6.5 GB
| 34 Billion| ~68 GB | ~34 GB | ~17 GB
| 70 Billion| ~140 GB| ~70 GB | ~35 GB
|===
****


=== 2. The KV Cache (Context Memory)
The Key-Value (KV) Cache stores attention data for the sequence being processed. For modern LLMs, this is often the largest and most volatile consumer of VRAM. Its size is not fixed; it grows dynamically based on your workload.

Key Drivers: The size of the KV Cache is directly proportional to:

 . Batch Size: The number of requests you process concurrently.

 . Context Length: The number of tokens (input + output) in each request.

Impact: For applications with long context windows (e.g., document summarization) or high batch sizes, the KV Cache can easily consume more VRAM than the model weights themselves.

[]
****
For Granite 3.3 8B Instruct, the KV Cache requires about 0.15625 MB per token. With a maximum context length of 131,072 tokens, this results in a KV Cache requirement of approximately 20 Gi.
****

=== 3. CUDA & System Overhead
This is the fixed cost of doing business on a GPU. It includes memory consumed by the NVIDIA CUDA kernels, the core PyTorch and vLLM libraries, and various system buffers required to manage the computation.

Estimated Cost: Budget an additional 10-20% of the model's weight VRAM for this overhead (as included in our formula).

=== 4. Model Activations
These are the intermediate values calculated during the model's forward pass. While their memory impact is far smaller than the KV Cache, they are a non-zero factor.

[#real-world-equation]
== The Real-World vRAM Equation

A practical formula for estimating your total memory requirement looks like this:

[source,text]
Total vRAM Needed â‰ˆ (VRAM for Model Weights & Overhead) + (VRAM for Max KV Cache)
Using our 8B model example:
19.0 Gi (Model & Overhead) + 20 Gi (KV Cache) = ~39 Gi

[IMPORTANT] 
.Don't Be Fooled by the "Sticker Price"
A model's advertised size is not its final cost in production. A 13B parameter model might list a ~26 GB requirement for its FP16 weights, suggesting it could fit on a 32 GB GPU. However, with a large batch size and long context window for the KV Cache, the actual VRAM requirement can easily exceed 40 GB.

Golden Rule: Always profile your specific use case with realistic batch sizes and context lengths. Never select hardware based solely on the VRAM needed for model weights.

[NOTE]
****
*Exercise: KV Cache Estimation*

An NVIDIA A10G or L4 GPU has 24 GB of vRAM. Given that the Granite 3.3 8B model requires ~19.0 Gi for its weights and overhead, what is the maximum context length (in tokens) you could configure for the KV Cache to ensure the entire workload fits on the device? ( using 0.15625 MB per token, 95% GPU usage )
****


[#sizing-tools]
== Sizing Tools and Strategy

=== Sizing Spreadsheet
To simplify these calculations, Red Hat Services has created a spreadsheet to help with sizing estimates for various LLMs.

https://red.ht/llm-sizing[LLM Sizing and TCO Calculator^]

Using the "Model Sizing" tab, you can select from a list of popular models to perform a sizing calculation. The spreadsheet provides hardware recommendations based on the model's default precision and maximum context length. You can override the context length to see how it impacts the vRAM requirements.

[NOTE]
****
*Exercise: Model Sizing Quantization Comparison*

Make a copy of the spreadsheet and perform a sizing calculation for the Llama-3-70B model. Compare the vRAM requirements for the un-quantized FP16 version versus the INT4 quantized version provided by Red Hat.
****



=== A Budget-Conscious Project Strategy
For customer Proof-of-Concept (PoC) projects with limited budgets, a strategic workflow is recommended.

.   Prioritize Optimized Models: Filter your model search to those that provide quantized versions. A quantized 13B model can often outperform a non-quantized 7B model while fitting in the same 24GB memory budget.
.   Validate and Iterate: Deploy your chosen model and benchmark its performance and real-world VRAM consumption. Be prepared to test different models to find the optimal balance of speed, accuracy, and cost.

.Annual Cloud Cost Estimates
[options="header"]
|===
| VRAM per GPU | Example AWS Instance | Estimated Annual Cost | Notes
| 24 GB | g6.4xlarge | $7,000 - $8,000 | Good performance-per-dollar for initial projects
| 48 GB | g6e.2xlarge | $12,000 - $14,000 | For larger models or higher-throughput scenarios
| 192 GB (4x48) | g6e.12xlarge | $55,000 - $65,000 | For multi-model serving or very large models
| 640 GB (8x80) | p5.48xlarge | $240,000+ | Enterprise scale (based on on-demand monthly cost)
|===
[IMPORTANT]
All costs are subject to change and are provided as estimates only. They should not be used for official customer quotes.

[#tensor-parallelism]
== When Your Model is Too Big for One GPU

You've been asked to deploy a powerful new Large Language Model, but when you try to load it, you hit the most common roadblock in AI infrastructure: the CUDA out of memory error.

This is where GPU aggregation strategies become essential. The first and most common strategy for this scenario is Tensor Parallelism.


=== Tensor Parallelism

Tensor Parallelism solves the memory problem by splitting a model's layers horizontally across multiple GPUs within a single server.

In technical terms, each GPU holds a "shard" or a slice of the model's weight matrices. They process their portion of the data in parallel and then use a high-speed interconnect to exchange the results, a process known as an all-reduce operation.


=== When to Use Tensor Parallelism

The rule for using Tensor Parallelism is simple and prescriptive.

[NOTE] .Guideline
Use Tensor Parallelism when your AI model is too large to fit on a single GPU, but it can fit within the combined memory of all GPUs in a single server.
It is the ideal solution for scaling up within the bounds of a single, powerful, multi-GPU node.

'''

=== Practical Configuration with vLLM

vLLM makes it simple to enable Tensor Parallelism. You just need to specify how many GPUs you want to use.

==== Example: Deploying Llama 3 70B on a server with 2 GPUs
A 70-billion parameter model like Llama 3 requires ~140GB of memory for its weights, which will not fit on a single 80GB H100 GPU. However, it will fit across two. You would configure vLLM with tensor_parallel_size=2.

==== Command-Line Configuration

When launching vLLM from the command line, use the --tensor-parallel-size argument.

[source,bash]
Launch vLLM using 2 GPUs for Tensor Parallelism
python -m vllm.entrypoints.api_server
--model "meta-llama/Llama-3-70B-Instruct" 
--tensor-parallel-size 2

==== Python Code Configuration

When using vLLM within your Python application, set the tensor_parallel_size in the EngineArgs.

[source,python]
from vllm import EngineArgs, LLMEngine
engine_args = EngineArgs( model="meta-llama/Llama-3-70B-Instruct", tensor_parallel_size=2 )