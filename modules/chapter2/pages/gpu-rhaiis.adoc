= GPUs for AI Inference

[#introduction]
== Introduction

Successfully deploying a Large Language Model is only half the battle. To build a cost-effective and performant AI platform, you must accurately match the model's requirements to the available hardware. Simply throwing oversized, expensive GPUs at every problem is a wasteful and ineffective strategy that leads to high costs and underutilized resources.

This module equips you with the fundamental knowledge and tools to analyze an LLM's requirements and make informed decisions about the necessary GPU infrastructure. You will learn how to estimate the GPU memory needed for a model including its KV Cache. Understand the impact of quantization, and use sizing tools to plan for future deployments.

image::intro_gpu_size.gif[.Objectives, 600]

[#lab-story]
== Lab Story

The Platform Engineering team at "InnovateForward Corp" is getting requests to deploy a variety of new models. The business wants to know: "What hardware do we need to run the new 70-billion-parameter model?" and "Can we run this smaller, optimized model on our existing A10G GPUs to save costs?"

To answer these questions confidently, the team needs to move beyond guesswork. They must learn how to calculate the video RAM (vRAM) footprint of a given model, including its parameters and the KV Cache. Mastering these calculations will enable them to provide accurate hardware recommendations, optimize resource utilization, and justify infrastructure costs to leadership.

[#understanding-vram]
==  The True Cost Blueprint

=== 1. Model Weights (The Baseline Cost)

This is the memory needed to load the model's parameters. It's a function of the model size (billions of parameters) and the numerical precision used to store each parameter.

Calculation:
A formula to estimate the memory for model weights, including overhead:

....
M = ((P * 4b) / (32 / Q)) * 1.2
....

[cols="1,4"]
|===
| Symbol | Description
| M | Total GPU memory required for the model weights (in Gi).
| P | The number of parameters in the model (e.g., 8 billion).
| 4b (bytes) | The size of a full-precision parameter in bytes (FP32).
| Q | The number of bits used for the model's precision type after quantization (e.g., 16 for FP16, 8 for INT8).
| 1.2| A multiplier representing a ~20% overhead for loading additional components into GPU memory.
|===

For example, let's calculate the requirement for the *ibm-granite/granite-3.3-8b-instruct model*, which has *8 billion parameters and uses the FP16 data type (16 bits)*.

[source,bash]
Formula: (((Parameters * 4 bytes) / (32 / Quantization_Bits)) * 1.2)
Calculation: (((8 * 4) / (32 / 16)) * 1.2) / = ~19 Gi


[]
****
*Quantization*

While not directly part of the sizing calculation, Quantization is a technique used to reduce the size and improve the performance of an LLM by converting its weights to a lower-precision data type. While many models default to FP16 (16 bits), it's common to quantize them to INT8 (8 bits) or even INT4 (4 bits) to significantly reduce their memory footprint. *Quantization is the most direct way to reduce the baseline memory cost.*

.Estimated vRAM for Model Weights by Precision
[options="header"]
|===
| Model Size (Parameters) | FP16 / BF16 (16-bit) | INT8 (8-bit) | INT4 (4-bit)
| 1 Billion | ~2 GB | ~1 GB | ~0.5 GB
| 3 Billion | ~6 GB | ~3 GB | ~1.5 GB
| 8 Billion | ~16 GB | ~8 GB | ~4 GB
| 13 Billion| ~26 GB | ~13 GB | ~6.5 GB
| 34 Billion| ~68 GB | ~34 GB | ~17 GB
| 70 Billion| ~140 GB| ~70 GB | ~35 GB
|===
****


=== 2. The KV Cache (Context Memory)
The Key-Value (KV) Cache stores attention data for the sequence being processed. For modern LLMs, this is often the largest and most volatile consumer of VRAM. Its size is not fixed; it grows dynamically based on your workload.

Key Drivers: The size of the KV Cache is directly proportional to:

 . Batch Size: The number of requests you process concurrently.

 . Context Length: The number of tokens (input + output) in each request.

Impact: For applications with long context windows (e.g., document summarization) or high batch sizes, the KV Cache can easily consume more VRAM than the model weights themselves.

[]
****
For *ibm-granite/granite-3.3-8b-instruct model*, the KV Cache requires about 0.15625 MB per token. With a maximum context length of 131,072 tokens, this results in a KV Cache requirement of approximately 20 Gi.
****

=== 3. CUDA & System Overhead
This is the fixed cost of doing business on a GPU. It includes memory consumed by the NVIDIA CUDA kernels, the core PyTorch and vLLM libraries, and various system buffers required to manage the computation.

Estimated Cost: Budget an additional 10-20% of the model's weight VRAM for this overhead (as included in our formula).

=== 4. Model Activations
These are the intermediate values calculated during the model's forward pass. While their memory impact is far smaller than the KV Cache, they are a non-zero factor.

[#real-world-equation]
== The Real-World vRAM Equation

A practical formula for estimating your total memory requirement looks like this:

[source,text]
Total vRAM Needed â‰ˆ (VRAM for Model Weights & Overhead) + (VRAM for Max KV Cache)
Using our 8B model example:
19.0 Gi (Model & Overhead) + 20 Gi (KV Cache) = ~39 Gi

[IMPORTANT] 
.Don't Be Fooled by the "Sticker Price"
A model's advertised size is not its final cost in production. A 13B parameter model might list a ~26 GB requirement for its FP16 weights, suggesting it could fit on a 32 GB GPU. However, with a large batch size and long context window for the KV Cache, the actual VRAM requirement can easily exceed 40 GB.

Golden Rule: Always profile your specific use case with realistic batch sizes and context lengths. Never select hardware based solely on the VRAM needed for model weights.

[NOTE]
****
*Exercise: KV Cache Estimation*

An NVIDIA A10G or L4 GPU has 24 GB of vRAM. Given that the *ibm-granite/granite-3.3-8b-instruct model* requires ~19.0 Gi for its weights and overhead, what is the maximum context length (in tokens) you could configure for the KV Cache to ensure the entire workload fits on the device? ( using 0.15625 MB per token, 95% GPU usage )
****




[#tensor-parallelism]
== When Your Model is Too Big for One GPU

You've been asked to deploy a powerful new Large Language Model, but when you try to load it, you hit the most common roadblock in AI infrastructure: the CUDA out of memory error.

This is where GPU aggregation strategies become essential. The first and most common strategy for this scenario is Tensor Parallelism.


=== Tensor Parallelism

Tensor Parallelism solves the memory problem by splitting a model's layers horizontally across multiple GPUs within a single server.

In technical terms, each GPU holds a "shard" or a slice of the model's weight matrices. They process their portion of the data in parallel and then use a high-speed interconnect to exchange the results, a process known as an all-reduce operation.


=== When to Use Tensor Parallelism

The rule for using Tensor Parallelism is simple.

[]
****
Use Tensor Parallelism when your AI model is too large to fit on a single GPU, but it can fit within the combined memory of all GPUs in a single server.
It is the ideal solution for scaling up within the bounds of a single, powerful, multi-GPU node.
****

'''

=== vLLM Configuration

vLLM makes it simple to enable Tensor Parallelism. You just need to specify how many GPUs you want to use.

==== Example: Deploying Llama 3 70B on a server with 2 GPUs
A 70-billion parameter model like Llama 3 requires ~140GB of memory for its weights, which will not fit on a single 80GB H100 GPU. However, it will fit across two. You would configure vLLM with tensor_parallel_size=2.

==== Command-Line Configuration

When launching vLLM from the command line, use the --tensor-parallel-size argument.

[source,bash]
Launch vLLM using 2 GPUs for Tensor Parallelism
python -m vllm.entrypoints.api_server
--model "meta-llama/Llama-3-70B-Instruct" 
--tensor-parallel-size 2

==== Python Code Configuration

When using vLLM within your Python application, set the tensor_parallel_size in the EngineArgs.

[source,python]
from vllm import EngineArgs, LLMEngine
engine_args = EngineArgs( model="meta-llama/Llama-3-70B-Instruct", tensor_parallel_size=2 )