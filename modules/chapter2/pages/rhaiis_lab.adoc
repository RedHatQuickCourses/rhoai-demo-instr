= Lab: Model Serving with Red Hat AI Inference Server


== Lab Overview and Objectives

This segment will walk you through the essential first steps of deploying an LLM, verifying its operation, and performing basic performance tuning based on your available hardware resources.

You will begin with a pre-configured Red Hat Enterprise Linux (RHEL) environment that already has the necessary NVIDIA drivers installed. Your focus will be entirely on the deployment and management of the RHAIIS container.


== Stop the Existing AI Model.

Our first task is to get our own baseline model up and running. 

. We first need to stop the running lab model to free up the GPU. In the top terminal run:
+
[source,bash]
----
sudo systemctl stop rhaiis.service
----

To make sure everything is working correctly, let's run a quick test. This command will start a temporary container and run `nvidia-smi` inside of it. If it works, you'll see details about your GPU. Keep this handy, it's great way to test container access to GPUs across environments. 

Run this command in **Terminal 1**:

```bash
sudo podman run --rm --device nvidia.com/gpu=all \
  docker.io/nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

.Nvidia GPU Status
image::nvidia_smi.png[Nvidia GPU Status]
You should see an output similar to the one you'd see if you ran `nvidia-smi` on the host, showing your GPU model, memory usage, and driver version.

== Deploying Your First Model


===  1. Create a Cache Directory

Create a local directory to cache the downloaded model. This saves time on subsequent launches.
[source,bash]
----
mkdir -p $HOME/rhaiis-cache
----

=== 2. Run the RHAIIS Container

This is the core of our lab. We're going to launch the RHAIIS container image and tell it to serve the `ibm-granite/granite-3.3-2b-instruct` model.

====

This command might look a bit complex, but let's break down the important parts:

  * *`--device nvidia.com/gpu=all`:* This gives the container access to all available GPUs.
  * *`--security-opt=label=disable`:* This is important for SELinux systems to allow the container to access local files.
  * *`--shm-size=4GB -p 8000:8000`:* This allocates shared memory and maps the container's port 8000 to the host's port 8000, so we can access the API.
  * *`--env "HUGGING_FACE_HUB_TOKEN=$HFTOKEN"`:* We're passing our Hugging Face token into the container.
  * *`-v $HOME/rhaiis-cache:/opt/app-root/src/.cache`:* This mounts our local cache directory, so the model only needs to be downloaded once.
  * *`registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1`:* This is the specific RHAIIS container image we're using.
  * *`--model ibm-granite/granite-3.3-2b-instruct`:* This tells the server which model to download and serve.

====


In **Terminal 1**, copy and paste the following command and press Enter. This will take some time as the model is downloaded and loaded onto the GPU.



[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4GB -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  --env "HF_HUB_OFFLINE=0" \
  --env=VLLM_NO_USAGE_STATS=1 \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct
----

=== 3. Monitor GPU Memory

There are a few tools for monitoring GPU memory. Since we are using Nvidia GPUs, where using their tools.

The `nvidia-smi` command is your primary tool for monitoring the GPU. We saw this demonstrated in our quick GPU test earlier. For this part of the lab, let's use the `nvtop` command.

. In your second terminal, run `nvtop`  to see live updates.
+
[source,bash]
----
nvtop
----

. Keep an eye on the Memory-Usage box. It will show you how much of the total available VRAM is being used as the model loads its weights and initializes the KV Cache, eventually consuming most of the GPU's memory. This initial usage represents the baseline VRAM consumption for the model with default settings.

.nvtop command status
image::nvtop.png[Nvidia GPU Status, 480]





=== 4. Verify the Deployment

Once you see logs indicating "gRPC Server started on http://0.0.0.0:8033", the server is ready.

. In ** terminal 2**, stop the `nvtop` process by pressing CTRL-C.
* Do not close the terminal or end the process where the RHAIIS container is running.*

. In ** terminal 2**, Use `curl` to send a test prompt to the server's completions endpoint.
+
[source,bash]
----
curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What are the key benefits of using Red Hat AI Inference Server?",
  "model": "ibm-granite/granite-3.3-2b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text
----

You should now see a standard, formatted response from the model, which confirms that the inference server is working correctly.

Feel free to change the prompt and try a few different queries to experiment with the model.

'''

== 4. Monitoring and Tuning VRAM Usage

Understanding and managing GPU memory is the most critical skill for serving LLMs efficiently. Now that we validated it's working . Let's see how much VRAM our model is using and how to tune it.

=== Monitor GPU Memory

This time let's use the  `nvidia-smi` command is your primary tool for monitoring the GPU.

. In your second terminal, run `nvidia-smi` in watch mode to see live updates.
+
[source,bash]
----
watch -n 1 nvidia-smi
----

. Observe the **Memory-Usage** column. It will show how much VRAM is being used out of the total available (e.g., `20052MiB / 23028MiB`). This is the baseline VRAM consumption for this model with default settings.

Press Ctrl-C to stop exist the nvidia smi command.

The other command available in this lab is the `nvtop` command. I like this command better because of the graph style concumption metrics provided for GPU memory usage and GPU processing usage. 



===  Tune for Maximum Context Length

The `--max-model-len` argument controls the maximum number of tokens (input prompt + generated output) a request can handle. A larger context length requires more VRAM. Let's find the sweet spot for our GPU.

. Stop the running RHAIIS container by pressing `Ctrl+C` in its terminal.

. Relaunch the server, this time adding the `--max-model-len` argument. Let's start with a value of `80000`.
+
[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4GB -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  --env "HF_HUB_OFFLINE=0" \
  --env=VLLM_NO_USAGE_STATS=1 \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --max-model-len 80000 <1>
----
<1> Limits the model's context length to 80000 tokens from the max 128,000 tokens.


Once the server is running, check your `nvidia-smi`, nvtop watch windows. You should see a noticeable decrease in VRAM usage. 

In reality,  what happens in this case is that the inference engine does limit the models max context lgenth to 80K, however, the RHAIIS (vLLM) engine still consumed all the available GPU memory it could based on the .90 (90%) utilization default setting. 

Let's reduce this memory setting next to see how this effects our GPU memory consumption. 

=== Fine-Tuning GPU Memory Utilization

The most direct way to *control the memory vLLM reserves* is with the `--gpu-memory-utilization` flag. It takes a value between 0.0 and 1.0. The default is `0.9`, which reserves 90% of the GPU's VRAM for this vLLM instance.

. Stop the running container with `Ctrl+C`.

. Relaunch the server, setting the utilization to 70% to leave more memory for other processes if needed.
+
[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4GB -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  --env "HF_HUB_OFFLINE=0" \
  --env=VLLM_NO_USAGE_STATS=1 \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --gpu-memory-utilization 0.70 <1>
----
<1> Instructs the server to use a maximum of 70% of the available GPU memory.

. Observe the change in memory allocation in `nvidia-smi`. The amount of memory reserved by the server will now be lower. This is a key parameter for running in shared environments.

'''

== 5. Deploying an Alternative Model

Switching models with RHAIIS is simple. Let's deploy the `granite-3.1-8b-instruct` model.

. Stop the current container with `Ctrl+C`.

. Run the `podman` command again, but change the value of the `--model` argument.
+
[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4GB -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  --env "HF_HUB_OFFLINE=0" \
  --env=VLLM_NO_USAGE_STATS=1 \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model RedHatAI/granite-3.1-8b-instruct <1>
----
<1> We've switched to the Granite 3.1- 8b model. The server will download it if it's not already in the cache.

. Once the server is running, test it with a new `curl` request. **Remember to update the model name in your request body.**
+
[source,bash]
----
curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What is the IBM Granite series of models?",
  "model": "RedHatAI/granite-3.1-8b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text
----

You have now successfully deployed and tested two different validated models, demonstrating the flexibility of the platform.

'''


== 6. Trouble Shooting the Larger Model

====

If you deployed your lab using the suggested environment, it's highly likely the larger Granite-8B model just crashed during loading. If you were monitoring with nvtop, you would have seen the GPU usage drop to zero.

This was expected. Earlier in the lab guide, you were asked to calculate the context length for the Granite 8B model. Let's revisit that information...

[NOTE]
****
*Exercise: KV Cache Estimation*

An NVIDIA A10G or L4 GPU has 24 GB of vRAM. Given that the *RedHatAI/granite-3.1-8b-instruct* requires ~19.0 Gi for its weights and overhead, what is the maximum context length (in tokens) you could configure for the KV Cache to ensure the entire workload fits on the device? ( using 0.15625 MB per token, 95% GPU usage )
****

*You will need to apply this answer as a argument / setting in order to launch the Granite-3.1-8B model successfully.*


[TIP]
Start small and during a successful launch, the engine will display the max token config for the available memory. 

====

== 7. Wrapping up this Experience

I know, I know you left you hanging on this last lab exercise, we'll add an arcade that shows the answer soon, until then everything you need is in this course.

The key points in this lab were:

Gain hands-on experience with the deployment of Red Hat AI Inference Server. 

You learned how to calculate a model GPU requirements, deploy a model, test its functionality, monitor its resource consumption, and tune its performance based on available VRAM. 


== 8. Lab Cleanup

To stop the services and clean up your environment, simply stop the running container.

. In the terminal where RHAIIS is running, press `Ctrl+C`.
.. The `--rm` flag used in the `podman run` command ensures the container is automatically removed upon exit.
. In the terminal where nvtop nvidia-smi is running, press `Ctrl+C`.
 .Delete the services on the Demo Platform. 

'''



