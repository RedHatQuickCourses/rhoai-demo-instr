= Lab: Model Serving with Red Hat AI Inference Server


== 1. Lab Overview and Objectives

This segment will walk you through the essential first steps of deploying an LLM, verifying its operation, and performing basic performance tuning based on your available hardware resources.

You will begin with a pre-configured Red Hat Enterprise Linux (RHEL) environment that already has the necessary NVIDIA drivers installed. Your focus will be entirely on the deployment and management of the RHAIIS container.


== 2. Deploying Your First Model

Our first task is to get a baseline model up and running. 

It's not on Red Hat validated models list, but the ibm-granite/granite-3.3-2b-instruct is small ~3B parameter model that have proven itself time and again for my experiments. So let's start with this model and then you tell me what your favorite model is. 


. Create a local directory to cache the downloaded model. This saves time on subsequent launches.
+
[source,bash]
----
mkdir -p $HOME/rhaiis-cache
----

To make sure everything is working correctly, let's run a quick test. This command will start a temporary container and run `nvidia-smi` inside of it. If it works, you'll see details about your GPU.

Run this command in **Terminal 1**:

```bash
sudo podman run --rm --device nvidia.com/gpu=all \
  docker.io/nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

You should see an output similar to the one you'd see if you ran `nvidia-smi` on the host, showing your GPU model, memory usage, and driver version.

=== 3. Run the RHAIIS Container

This is the core of our lab. We're going to launch the RHAIIS container image and tell it to serve the `ibm-granite/granite-3.3-2b-instruct` model.

This command might look a bit complex, but let's break down the important parts:

  * `--device nvidia.com/gpu=all`: This gives the container access to all available GPUs.
  * `--security-opt=label=disable`: This is important for SELinux systems to allow the container to access local files.
  * `--shm-size=4GB -p 8000:8000`: This allocates shared memory and maps the container's port 8000 to the host's port 8000, so we can access the API.
  * `--env "HUGGING_FACE_HUB_TOKEN=$HFTOKEN"`: We're passing our Hugging Face token into the container.
  * `-v $HOME/rhaiis-cache:/opt/app-root/src/.cache`: This mounts our local cache directory, so the model only needs to be downloaded once.
  * `registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1`: This is the specific RHAIIS container image we're using.
  * `--model ibm-granite/granite-3.3-2b-instruct`: This tells the server which model to download and serve.

In **Terminal 1**, copy and paste the following command and press Enter. This will take some time as the model is downloaded and loaded onto the GPU.

[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct
----


The server will now start. The first launch will take several minutes as it downloads the model into the cache directory. Subsequent launches will be much faster.

=== 4. Verify the Deployment

Once you see logs indicating "Uvicorn running on http://0.0.0.0:8000", the server is ready.

. In ** terminal 2** . Do not close the terminal or end the process where the RHAIIS container is running.

. Use `curl` to send a test prompt to the server's completions endpoint.
+
[source,bash]
----
curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What are the key benefits of using Red Hat AI Inference Server?",
  "model": "ibm-granite/granite-3.3-2b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text
----

You should see standard formatted response generated by the model, confirming that the inference server is working correctly.

'''

== 4. Monitoring and Tuning VRAM Usage

Understanding and managing GPU memory is the most critical skill for serving LLMs efficiently. Let's see how much VRAM our model is using and how to tune it.

=== Monitor GPU Memory

The `nvidia-smi` command is your primary tool for monitoring the GPU.

. In your second terminal, run `nvidia-smi` in watch mode to see live updates.
+
[source,bash]
----
watch -n 1 nvidia-smi
----

. Observe the **Memory-Usage** column. It will show how much VRAM is being used out of the total available (e.g., `8152MiB / 23028MiB`). This is the baseline VRAM consumption for this model with default settings.



===  Tune for Maximum Context Length

The `--max-model-len` argument controls the maximum number of tokens (input prompt + generated output) a request can handle. A larger context length requires more VRAM. Let's find the sweet spot for our GPU.

. Stop the running RHAIIS container by pressing `Ctrl+C` in its terminal.

. Relaunch the server, this time adding the `--max-model-len` argument. Let's start with a value of `4096`.
+
[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --max-model-len 4096 <1>
----
<1> Limits the model's context length to 4096 tokens.

. Once the server is running, check your `nvidia-smi` watch window. You should see a noticeable decrease in VRAM usage.

=== Fine-Tuning GPU Memory Utilization

The most direct way to *control the memory vLLM reserves* is with the `--gpu-memory-utilization` flag. It takes a value between 0.0 and 1.0. The default is `0.9`, which reserves 90% of the GPU's VRAM for this vLLM instance.

. Stop the running container with `Ctrl+C`.

. Relaunch the server, setting the utilization to 85% to leave more memory for other processes if needed.
+
[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --gpu-memory-utilization 0.85 <1>
----
<1> Instructs the server to use a maximum of 85% of the available GPU memory.

. Observe the change in memory allocation in `nvidia-smi`. The amount of memory reserved by the server will now be lower. This is a key parameter for running in shared environments.

'''

== 5. Deploying an Alternative Model

Switching models with RHAIIS is simple. Let's deploy the `granite-3.1-8b-instruct` model.

. Stop the current container with `Ctrl+C`.

. Run the `podman` command again, but change the value of the `--model` argument.
+
[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model RedHatAI/granite-3.1-8b-instruct <1>
----
<1> We've switched to the Granite model. The server will download it if it's not already in the cache.

. Once the server is running, test it with a new `curl` request. **Remember to update the model name in your request body.**
+
[source,bash]
----
curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What is the IBM Granite series of models?",
  "model": "RedHatAI/granite-3.1-8b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text
----

You have now successfully deployed and tested two different validated models, demonstrating the flexibility of the platform.

'''

== 6. Lab Cleanup

To stop the services and clean up your environment, simply stop the running container.

. In the terminal where RHAIIS is running, press `Ctrl+C`.

. The `--rm` flag used in the `podman run` command ensures the container is automatically removed upon exit.

'''

== 7. Conclusion

In this lab, you gained hands-on experience with the core workflow of Red Hat AI Inference Server. You learned how to deploy a model, test its functionality, monitor its resource consumption, and tune its performance based on available VRAM.

**Key Takeaways:**

* RHAIIS deployment is managed with a single, configurable `podman run` command.
* `nvidia-smi` is essential for monitoring VRAM usage.
* The `--gpu-memory-utilization` and `--max-model-len` arguments are your primary tools for memory management.
* Switching between different validated models is as simple as changing the `--model` parameter.

In the next lab, we will build on this foundation to explore multi-GPU deployments and advanced customizations.