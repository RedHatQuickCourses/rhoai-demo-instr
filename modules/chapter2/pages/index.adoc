= Red Hat AI Inference Server

Welcome to the world of **Red Hat AI Inference Server (RHAIIS)**. This hands-on lab is your introduction to a powerful, supported platform for deploying large language models (LLMs) with high performance and flexibility across any hybrid cloud environments. As a key component of the Red Hat AI platform, RHAIIS is available as a standalone product and is included in Red Hat OpenShift AI and Red Hat Enterprise Linux AI (RHEL AI).

RHAIIS provides a hardened, supported distribution of the **vLLM** software via prepackaged container. This open-source project is renowned for its high-throughput and memory-efficient performance, achieved through innovative techniques like PagedAttention and continuous batching.


== Key Features of RHAIIS

Red Hat AI Inference Server leverages several advanced techniques to deliver high-performance LLM serving:

=== vLLM Core and Advanced Parallelism

At its heart, RHAIIS is built on the vLLM serving engine, which maximizes throughput and minimizes latency for LLM inference.

* **PagedAttention**: This technique optimizes GPU memory management by virtualizing the key-value (KV) cache for each request. It addresses memory wastage and lowers costs by consuming less memory.
* **Continuous Batching**: It processes requests as they arrive instead of waiting for a full batch to be accumulated, reducing latency and increasing throughput.
* **Tensor Parallelism (TP)**: This feature distributes LLM workloads across multiple GPUs, within a single node, to reduce latency and increase computational throughput. You can configure this using the `--tensor-parallel-size` argument.
// * **Pipeline Parallelism (PP)**: This stages sequential groups of model layers across different GPUs or nodes, which is crucial for fitting models that are too large for a single multi-GPU node.
* **Expert Parallelism (EP)**: RHAIIS includes specialized optimizations for efficiently handling Mixture of Experts (MoE) model architectures.
// * **Data Parallelism (DP)**: This routes individual requests to different vLLM engines.

=== Model Optimization and Compression

RHAIIS integrates the **LLM Compressor** library, a Developer Preview feature that provides a unified framework for optimizing and compressing large language models before inferencing.

* **Quantization**: This technique converts model weights and activations to lower-bit formats (such as INT8) to reduce memory usage.
* **Sparsity**: It sets a portion of model weights to zero, enabling more efficient computation.
//* **Compression**: This shrinks the saved model file size, ideally with minimal impact on performance.

[NOTE]
For RHAIIS 3.2, LLM Compressor is a Developer Preview feature and is not included in the standard container image. The product does, however, support specific quantization variants like FP8 (W8A8) and GGUF, particularly for AMD GPUs.

=== Deployment Portability and Multi-Accelerator Support

RHAIIS is delivered as a container image downloaded from Red Hat Registry, which enables deployment flexibility and  across various connected environments.

.Supported Deployment Environments
[cols="2,3,4", options="header"]
|===
|Environment
|Supported Versions
|Deployment Notes

|OpenShift Container Platform (self-managed)
|4.14-4.19
|Deploy on bare-metal hosts or virtual machines.

|Red Hat OpenShift Service on AWS (ROSA)
|4.14-4.19
|Requires ROSA STS cluster with GPU-enabled P5 or G5 node types.

|Red Hat Enterprise Linux (RHEL)
|9.2-10.0
|Deploy on bare-metal hosts or virtual machines.
|===

The server is engineered with robust multi-accelerator support for a diverse range of hardware. RHAIIS 3.2 supports the following data center-grade accelerators:

* **NVIDIA GPUs**: A2, A10, A16, A30, A40, A100, L4, L40, L40S, H100, H200, B200, and RTX PRO 6000 Blackwell Server Edition.
* **AMD GPUs**: Instinct M1210 and Instinct M130OX.
* **Google TPUs**: TPU v6e (Developer Preview).
