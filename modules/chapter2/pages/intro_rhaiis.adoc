= Model Serving with Red Hat AI Inference Server

== Introduction

Congratulations on your excellent choice to take this training on model serving with:

**Red Hat AI Inference Server (RHAIIS)**. 

This hands-on guide will introduce you to using Red Hat AI Inference Server (RHAIIS) 3.2.0—a key component of the Red Hat AI platform—by walking you through the essential first concepts, then deploying and verifying an LLM and performing basic performance tuning based on your available hardware. 

Available as a standalone product and included in Red Hat OpenShift AI and Red Hat Enterprise Linux AI, this supported platform provides a powerful and flexible solution for deploying high-performance large language models across any hybrid cloud environment.

RHAIIS provides a hardened, supported distribution of the **vLLM** software via pre-packaged containers, depending on GPU type. The vLLM open-source project is renowned for its high-throughput and memory-efficient performance, achieved through innovative techniques like PagedAttention and continuous batching.

== Overview and Objectives

Upon completing this lab, you will be able to:

 * Deploy RHAIIS with a validated model using a single Podman command.
 * Verify the model is serving correctly by interacting with its API.
 * Monitor the GPU's video memory (VRAM) usage in real-time.
 * Tune server parameters to control memory consumption and context length.
 * Deploy and test an alternative model to see the platform's flexibility.

== Prerequisites

Your lab environment has been pre-configured with the following:

* A Red Hat Enterprise Linux 9.x system with a valid subscription.
* An attached and configured NVIDIA data center GPU with drivers installed.
* Podman and the NVIDIA Container Toolkit are pre-installed.
* Credentials for **Red Hat account** to access `registry.redhat.io`. (Provided for this experience)
* A **Hugging Face account** with a User Access Token with read permissions. (Provided for this experience)






