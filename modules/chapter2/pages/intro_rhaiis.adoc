= Sizing for GPUs

[%hardbreaks]

== Introduction

Successfully deploying a Large Language Model is only half the battle. To build a cost-effective and performant AI platform, you must be able to accurately match the model's requirements to the available hardware. Simply throwing oversized, expensive GPUs at every problem is a wasteful and ineffective strategy that leads to high costs and underutilized resources.

This module equips you with the fundamental knowledge and tools to analyze an LLM's requirements and make informed decisions about the necessary GPU infrastructure. You will learn how to estimate the memory needed for a model and its KV Cache, understand the impact of quantization, and use sizing tools to plan for future deployments.

image::intro_gpu_size.gif[.Objectives, 600]

== Lab Story

The Platform Engineering team at "InnovateForward Corp" is getting requests to deploy a variety of new models. The business wants to know: "What hardware do we need to run the new 70-billion-parameter model?" and "Can we run this smaller, specialized model on our existing A10G GPUs to save costs?"

To answer these questions confidently, the team needs to move beyond guesswork. They must learn how to calculate the vRAM footprint of a given model, including its parameters and the KV Cache. Mastering these calculations will enable them to provide accurate hardware recommendations, optimize resource utilization, and justify infrastructure costs to leadership.

== Estimating Model Size

The first step in sizing a GPU is to calculate the amount of video RAM (vRAM) the model's weights will consume. The size of the model loaded into memory can be estimated using the following formula:

....
M = ((P * 4) / (32 / Q)) * 1.2
....

[cols="1,4"]
|===
| Symbol | Description
| *M*
| Total GPU memory required for the model weights (in bytes).

| *P*
| The number of parameters in the model (e.g., 8 billion).

| *4b*
| The size of a full-precision parameter in bytes (FP32).

| *Q*
| The number of bits used for the model's data type after quantization (e.g., 16 for FP16, 8 for INT8).

| *1.2*
| A multiplier representing a ~20% overhead for loading additional components into GPU memory.
|===

For example, let's calculate the requirement for the `ibm-granite/granite-3.3-8b-instruct` model, which has 8 billion parameters and uses the FP16 data type (16 bits).

[source,bash]
----
# Formula: (((Parameters * 4 bytes) / (32 / Quantization_Bits)) * 1.2) / Bytes_per_GB
(((8 * 4) / (32 / 16)) * 1.2) = 17.9 Gb
----

=== Quantization
*Quantization* is a technique used to reduce the size and improve the performance of an LLM by converting its weights to a lower-precision data type. While many models default to FP16 (16 bits), it's common to quantize them to INT8 (8 bits) or even INT4 (4 bits) to significantly reduce their memory footprint.

== Estimating KV Cache

Beyond the model's weights, the serving runtime (vLLM) also requires vRAM for the **KV Cache**. This cache stores the state of ongoing conversations, and its size is a critical factor in the model's ability to handle long contexts.

The memory required for the KV Cache is determined by two main factors:
. The memory required per token (this varies by model architecture).
. The maximum context length (number of tokens) you need to support for your use case.

For `Granite 3.3 8B Instruct`, the KV Cache requires about **0.15625 Mb per token**. With a maximum context length of 131,072 tokens, this results in a KV Cache requirement of approximately **20 Gb**.

Therefore, to run this model and support its maximum context length, our total estimated vRAM requirement is:
`17.9 Gb (Model) + 20 Gb (KV Cache) = ~38 Gb`

[NOTE]
====
*Exercise: KV Cache Estimation*

An NVIDIA A10G or L4 GPU has 24 Gb of vRAM. Given that the Granite 3.3 8B model requires ~17.9 Gb for its weights, what is the maximum context length (in tokens) you could configure for the KV Cache to ensure the entire workload fits on the device?
====

== Sizing Spreadsheet

To simplify these calculations, Red Hat Services has created a spreadsheet to help with sizing estimates for various LLMs.

https://red.ht/llm-sizing[LLM Sizing and TCO Calculator^]

Using the **"Model Sizing"** tab, you can select from a list of popular models to perform a sizing calculation. The spreadsheet provides hardware recommendations based on the model's default precision and maximum context length. You can override the context length to see how it impacts the vRAM requirements for your specific use case.

The **"Subs & Cost Modeling"** tab can help recommend cloud instance types and provides a high-level Total Cost of Ownership estimate for running the models on OpenShift.

[IMPORTANT]
Please keep in mind that all costs are subject to change and are provided as estimates only. They should not be used for official customer quotes.

[NOTE]
====
*Exercise: Model Sizing Quantization Comparison*

Make a copy of the spreadsheet and perform a sizing calculation for the Llama-3-70B model. Compare the vRAM requirements for the un-quantized FP16 version versus the INT4 quantized version provided by Red Hat.
====

== The Problem: Your Model is Too Big for One GPU

You've been asked to deploy a powerful new Large Language Model, but when you try to load it, you hit the most common roadblock in AI infrastructure: the `CUDA out of memory` error. The model's weights and its runtime KV cache are simply too large to fit into the memory of a single GPU, even a top-tier one like an A100 or H100.

This is where GPU aggregation strategies become essential. The first and most common strategy for this scenario is **Tensor Parallelism**.

'''

== How It Works: The "Team of Chefs" Analogy

Tensor Parallelism solves the memory problem by splitting a model's layers *horizontally* across multiple GPUs within a single server.

Imagine a team of chefs (your GPUs) tasked with preparing an incredibly complex recipe (a single layer of the AI model).

* **Instead of one chef doing all the work**, the recipe is split. Chef 1 handles the vegetables, Chef 2 handles the sauces, and so on.
* **They work simultaneously**, which dramatically speeds up the preparation time. This is how Tensor Parallelism reduces latency.
* **At the end**, they must quickly combine their finished components to create the final dish. This requires constant, high-speed communication between the chefs.

In technical terms, each GPU holds a "shard" or a slice of the model's weight matrices. They process their portion of the data in parallel and then use a high-speed interconnect to exchange the results, a process known as an `all-reduce` operation.

.A high-level view of Tensor Parallelism
image::tensor-parallelism-overview.png[Tensor Parallelism, 600]

'''

== The Critical Prerequisite: High-Speed Interconnect

The "communication between chefs" is the most critical part of this process. The performance of Tensor Parallelism is fundamentally dependent on the bandwidth of the connection between the GPUs.

[IMPORTANT]
====
Tensor Parallelism is designed to be used with high-speed, direct interconnects like **NVIDIA NVLink** or NVSwitch. Using it over a slower interconnect like standard PCIe will create a severe communication bottleneck, negating the performance benefits and potentially leading to slower results than using a single GPU.

**A consultant's key takeaway:** When designing a server for Tensor Parallelism, NVLink is not just a nice-to-have; it is a core requirement.
====

'''

== When to Use Tensor Parallelism

The rule for using Tensor Parallelism is simple and prescriptive.

[NOTE]
.Guideline
====
Use Tensor Parallelism when your AI model is **too large to fit on a single GPU**, but it **can fit within the combined memory of all GPUs in a single server**.
====

It is the ideal solution for scaling up within the bounds of a single, powerful, multi-GPU node.

'''

== Practical Configuration with vLLM

vLLM makes it simple to enable Tensor Parallelism. You just need to specify how many GPUs you want to use.

.Example: Deploying Llama 3 70B on a server with 2 GPUs
A 70-billion parameter model like Llama 3 requires ~140GB of memory, which will not fit on a single 80GB H100 GPU. However, it will fit across two.

You would configure vLLM with `tensor_parallel_size=2`.

=== Command-Line Configuration

When launching vLLM from the command line, use the `--tensor-parallel-size` argument.

```bash
# Launch vLLM using 2 GPUs for Tensor Parallelism
python -m vllm.entrypoints.api_server \
    --model "meta-llama/Llama-3-70B-Instruct" \
    --tensor-parallel-size 2

=== Python Code Configuration

When using vLLM within your Python application, set the tensor_parallel_size in the EngineArgs.

[]
from vllm import EngineArgs, LLMEngine

engine_args = EngineArgs(
    model="meta-llama/Llama-3-70B-Instruct",