= Red Hat AI Inference Server

== Key Features of RHAIIS

Red Hat AI Inference Server leverages several advanced techniques to deliver high-performance LLM serving:

=== vLLM Core and Advanced Parallelism

At its heart, RHAIIS is built on the vLLM serving engine, which maximizes throughput and minimizes latency for LLM inference.

* **PagedAttention**: This technique optimizes GPU memory management by virtualizing the key-value (KV) cache for each request. It addresses memory wastage and lowers costs by consuming less memory.
* **Continuous Batching**: It processes requests as they arrive instead of waiting for a full batch to be accumulated, reducing latency and increasing throughput.
* **Tensor Parallelism (TP)**: This feature distributes LLM workloads across multiple GPUs, within a single node, to reduce latency and increase computational throughput. You can configure this using the `--tensor-parallel-size` argument.
// * **Pipeline Parallelism (PP)**: This stages sequential groups of model layers across different GPUs or nodes, which is crucial for fitting models that are too large for a single multi-GPU node.
* **Expert Parallelism (EP)**: RHAIIS includes specialized optimizations for efficiently handling Mixture of Experts (MoE) model architectures.
// * **Data Parallelism (DP)**: This routes individual requests to different vLLM engines.

=== Model Optimization and Compression

RHAIIS integrates the **LLM Compressor** library, a Developer Preview feature that provides a unified framework for optimizing and compressing large language models before inferencing.

* **Quantization**: This technique converts model weights and activations to lower-bit formats (such as INT8) to reduce memory usage.
* **Sparsity**: It sets a portion of model weights to zero, enabling more efficient computation.
//* **Compression**: This shrinks the saved model file size, ideally with minimal impact on performance.

[NOTE]
For RHAIIS 3.2, LLM Compressor is a Developer Preview feature and is not included in the standard container image. The product does, however, support specific quantization variants like FP8 (W8A8) and GGUF, particularly for AMD GPUs.

=== Deployment Portability and Multi-Accelerator Support

RHAIIS is delivered as a container image downloaded from Red Hat Registry, which enables deployment flexibility and  across various connected environments.

.Supported Deployment Environments
[cols="2,3,4", options="header"]
|===
|Environment
|Supported Versions
|Deployment Notes

|OpenShift Container Platform (self-managed)
|4.14-4.19
|Deploy on bare-metal hosts or virtual machines. 

|Red Hat OpenShift Service on AWS (ROSA)
|4.14-4.19
|Requires ROSA STS cluster with GPU-enabled P5 or G5 node types.

|Red Hat Enterprise Linux (RHEL)
|9.2-10.0
|Deploy on bare-metal hosts or virtual machines.

|Linux (not RHEL)
|
|Supported under third‑party policy deployed on bare‑metal hosts or virtual machines. OpenShift Container Platform Operators are not required.

|Kubernetes (not OpenShift Container Platform)
|
|Supported under third‑party policy deployed on bare‑metal hosts or virtual machines.
|===

The server is engineered with robust multi-accelerator support for a diverse range of hardware. RHAIIS 3.2 supports the following data center-grade accelerators:


.Supported AI accelerators
[cols="2,3,4", options="header"]
|===
|AI accelerators
|
|

|NVIDIA GPUs:
|AMD GPUs:
|Google TPUs


|Ampere: A2, A10, A16, A30, A40, A1000, L40S
|AMD Instinct MI210
|TPU v6e (Developer Preview)

|Ada: L4, L40, L40S
|AMD Instinct MI300X
|

|Hopper: H100, H200, GH200
|
|

|Blackwell: B200, RTX PRO 6000 Blackwell Server Edition
|
|
|===
