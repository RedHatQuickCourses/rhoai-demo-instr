= Architecting GPU as a Service

Welcome to the foundational module of our Models-as-a-Service workshop. Before we can serve AI models, we must first build the high-performance, cost-effective infrastructure they will run on. This module provides the blueprint for transforming your GPU resources from siloed, underutilized assets into a centralized, elastic "GPU as a Service" platform on OpenShift AI.

== The Strategic Imperative: Unlocking GPU Value

Modern AI, especially Large Language Models (LLMs), depends on the immense computational power of GPUs like the NVIDIA A100 and H100. However, the traditional model of allocating dedicated GPUs per project is plagued with inefficiency, leading to inflated costs and bottlenecks that slow down innovation.

The solution is to treat GPUs not as project-specific hardware, but as a centrally managed, shared service. This paradigm shift delivers transformative value:

* **Significant Cost Reduction:** Maximize the utilization of your existing GPU fleet (from an average of 30-40% to over 80%).
* **Accelerated Access:** Enable developer self-service, allowing teams to provision GPU resources in minutes, not weeks.
* **Democratized Innovation:** Empower more teams to experiment and build AI applications without the barrier of hardware procurement.

== Core Architectural Capabilities

Building this platform requires two core sets of capabilities: *sharing* a single GPU among many smaller workloads and *aggregating* many GPUs for a single, massive workload.

=== Flexible GPU Sharing for Concurrent Workloads

GPU sharing techniques allow multiple workloads to run concurrently on a single physical GPU, maximizing utilization for development, testing, and smaller inference tasks.

Multi-Instance GPU (MIG):::
    Partitions a single physical GPU into up to seven smaller, fully isolated "virtual GPUs." Each instance has its own dedicated memory and compute, providing strict hardware-level isolation and predictable performance.

GPU Timeslicing:::
    A software-based technique where multiple workloads take turns accessing the GPU. It is highly effective but offers no memory isolation, which can lead to resource contention.

=== GPU Aggregation for Large-Scale AI Models

When a single model is too large to fit into one GPU's memory, aggregation techniques (or "distributed inference") are essential. This is the key to serving today's most powerful LLMs.

Tensor Parallelism:::
    Shards a model's layers across multiple GPUs, typically within a single node connected by a high-bandwidth interconnect like NVLink. This reduces latency by processing parts of the model concurrently.

Pipeline Parallelism:::
    Partitions the model into a sequence of stages, with each stage running on a different GPU or node. This is crucial for models so large they exceed the capacity of all GPUs within a single machine.

Data Parallelism:::
    Replicates the same model across multiple GPUs and distributes data batches among them. While primarily a training technique, it can be useful for isolating user request batches during inference.

Expert Parallelism:::
    A specialized technique for Mixture of Experts (MoE) models that assigns different "expert" sub-networks to dedicated GPUs, leveraging sparse activation patterns to scale efficiently.

== Putting Theory into Practice: Distributed Inference with vLLM

While these concepts apply broadly, our focus is on implementing them with vLLM, a high-performance inference engine. Serving large models with vLLM often leads to memory bottlenecks, such as the dreaded `CUDA out of memory` error. Distributed inference is the primary solution.

=== Choosing Your vLLM Distribution Strategy

The decision of which strategy to use depends on your model size, hardware, and performance goals.

**Single GPU (No Distributed Inference)**::
If your model fits comfortably in a single GPU's memory, this is the simplest option with the least overhead.

**Single-Node, Multi-GPU (Tensor Parallelism)**::
When your model exceeds single GPU capacity but fits within one node, use tensor parallelism. Set the `tensor_parallel_size` to the number of available GPUs on the node.
+
.Example Configuration: 4 GPUs in a single node
----
tensor_parallel_size = 4
----

**Multi-Node, Multi-GPU (Tensor + Pipeline Parallelism)**::
For models that exceed single-node capacity, combine tensor and pipeline parallelism. `tensor_parallel_size` is the number of GPUs *per node*, and `pipeline_parallel_size` is the number of nodes.
+
.Example Configuration: 16 GPUs across 2 nodes (8 GPUs per node)
----
tensor_parallel_size = 8
pipeline_parallel_size = 2
----

=== The "As a Service" Enablement Layer

Finally, to deliver a true self-service experience, we need an enablement layer on top of our GPU infrastructure. This includes:

* **API Gateway (e.g., Red Hat 3scale):** Provides a unified, secure "front door" for developers.
* **Identity and Access Management (IAM):** Authenticates users and authorizes access.
* **Observability:** Delivers critical logging and metrics for monitoring, cost allocation, and capacity planning.

== Useful Links

. Distributed Inference with vLLM - GPU Parallelism Techniques
+
https://developers.redhat.com/articles/2025/02/06/distributed-inference-with-vllm#gpu_parallelism_techniques_in_vllm[^]

. How We Optimized vLLM DeepSeek R1 - Open Infra Week Contributions
+
https://developers.redhat.com/articles/2025/03/19/how-we-optimized-vllm-deepseek-r1#open_infra_week_contributions[^]

. GPU Partitioning Guide
+
https://github.com/rh-aiservices-bu/gpu-partitioning-guide[^]