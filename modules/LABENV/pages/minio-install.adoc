= Lab 1: Foundational Model Serving with Red Hat AI Inference Server
:author: Red Hat AI Inference Server - Labs
:revdate: 2025-08-11
:toc: left
:toclevels: 3
:sectnums:
:source-highlighter: rouge
:icons: font

A guided, hands-on exercise for deploying and tuning your first Large Language Model with Red Hat AI Inference Server.

'''

== 1. Lab Overview and Objectives

Welcome to the hands-on lab for Red Hat AI Inference Server (RHAIIS). This guide will walk you through the essential first steps of deploying an LLM, verifying its operation, and performing basic performance tuning based on your available hardware resources.

In this lab, you will begin with a pre-configured Red Hat Enterprise Linux (RHEL) environment that already has the necessary NVIDIA drivers installed. Your focus will be entirely on the deployment and management of the RHAIIS container.

=== 1.1. Learning Objectives

Upon completing this lab, you will be able to:

* Deploy RHAIIS with a validated model using a single Podman command.
* Verify the model is serving correctly by interacting with its API.
* Monitor the GPU's video memory (VRAM) usage in real-time.
* Tune server parameters to control memory consumption and context length.
* Deploy and test an alternative model to see the platform's flexibility.

'''

== 2. Prerequisites

Your lab environment has been pre-configured with the following:

* A Red Hat Enterprise Linux 9.x system with a valid subscription.
* An attached and configured NVIDIA data center GPU with drivers installed.
* Podman and the NVIDIA Container Toolkit are pre-installed.

You will need to provide:

* Credentials for your **Red Hat account** to access `registry.redhat.io`.
* A **Hugging Face account** with a User Access Token (read permissions).

'''

== 3. Deploying Your First Model

Our first task is to get a baseline model up and running. We will use `RedHatAI/Llama-3.1-8B-Instruct`, a capable and widely used model.

=== 3.1. Authenticate and Prepare

Before running the server, you must log in to the Red Hat registry and configure your Hugging Face token.

. Log in to the Red Hat container registry using your account credentials.
+
[source,bash]
----
podman login registry.redhat.io
----

. Set your Hugging Face token as an environment variable. This allows the server to download the model from the Hugging Face Hub.
+
[source,bash]
----
# ⚠️ Replace <your_HF_token> with your actual token
export HF_TOKEN="<your_HF_token>"
----

. Create a local directory to cache the downloaded model. This saves time on subsequent launches.
+
[source,bash]
----
mkdir -p $HOME/rhaiis-cache
----

=== 3.2. Run the RHAIIS Container

Now, run the server with the `podman run` command.

[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model RedHatAI/Llama-3.1-8B-Instruct
----

The server will now start. The first launch will take several minutes as it downloads the model into the cache directory. Subsequent launches will be much faster.

=== 3.3. Verify the Deployment

Once you see logs indicating "Uvicorn running on http://0.0.0.0:8000", the server is ready.

. Open a **new terminal** or SSH session. Do not close the terminal where the container is running.

. Use `curl` to send a test prompt to the server's completions endpoint.
+
[source,bash]
----
curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What are the key benefits of using Red Hat AI Inference Server?",
  "model": "RedHatAI/Llama-3.1-8B-Instruct",
  "max_tokens": 150
}' | jq .choices[0].text
----

You should see a helpful, well-formatted response generated by the model, confirming that the inference server is working correctly.

'''

== 4. Monitoring and Tuning VRAM Usage

Understanding and managing GPU memory is the most critical skill for serving LLMs efficiently. Let's see how much VRAM our model is using and how to tune it.

=== 4.1. Monitor GPU Memory

The `nvidia-smi` command is your primary tool for monitoring the GPU.

. In your second terminal, run `nvidia-smi` in watch mode to see live updates.
+
[source,bash]
----
watch -n 1 nvidia-smi
----

. Observe the **Memory-Usage** column. It will show how much VRAM is being used out of the total available (e.g., `8152MiB / 23028MiB`). This is the baseline VRAM consumption for this model with default settings.



=== 4.2. Tune for Maximum Context Length

The `--max-model-len` argument controls the maximum number of tokens (input prompt + generated output) a request can handle. A larger context length requires more VRAM. [cite_start]Let's find the sweet spot for our GPU. [cite: 135]

. Stop the running container by pressing `Ctrl+C` in its terminal.

. Relaunch the server, this time adding the `--max-model-len` argument. Let's start with a value of `4096`.
+
[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model RedHatAI/Llama-3.1-8B-Instruct \
  --max-model-len 4096 <1>
----
<1> Limits the model's context length to 4096 tokens.

. Once the server is running, check your `nvidia-smi` watch window. You should see a noticeable increase in VRAM usage.

=== 4.3. Fine-Tuning GPU Memory Utilization

The most direct way to control the memory vLLM reserves is with the `--gpu-memory-utilization` flag. [cite_start]It takes a value between 0.0 and 1.0. [cite: 130] [cite_start]The default is `0.9`, which reserves 90% of the GPU's VRAM. [cite: 603]

. Stop the running container with `Ctrl+C`.

. Relaunch the server, setting the utilization to 85% to leave more memory for other processes if needed.
+
[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model RedHatAI/Llama-3.1-8B-Instruct \
  --gpu-memory-utilization 0.85 <1>
----
<1> Instructs the server to use a maximum of 85% of the available GPU memory.

. Observe the change in memory allocation in `nvidia-smi`. The amount of memory reserved by the server will now be lower. This is a key parameter for running in shared environments.

'''

== 5. Deploying an Alternative Model

Switching models with RHAIIS is simple. Let's deploy the `granite-3.1-8b-instruct` model.

. Stop the current container with `Ctrl+C`.

. Run the `podman` command again, but change the value of the `--model` argument.
+
[source,bash]
----
podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model RedHatAI/granite-3.1-8b-instruct <1>
----
<1> We've switched to the Granite model. The server will download it if it's not already in the cache.

. Once the server is running, test it with a new `curl` request. **Remember to update the model name in your request body.**
+
[source,bash]
----
curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What is the IBM Granite series of models?",
  "model": "RedHatAI/granite-3.1-8b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text
----

You have now successfully deployed and tested two different validated models, demonstrating the flexibility of the platform.

'''

== 6. Lab Cleanup

To stop the services and clean up your environment, simply stop the running container.

. In the terminal where RHAIIS is running, press `Ctrl+C`.

. The `--rm` flag used in the `podman run` command ensures the container is automatically removed upon exit.

'''

== 7. Conclusion

In this lab, you gained hands-on experience with the core workflow of Red Hat AI Inference Server. You learned how to deploy a model, test its functionality, monitor its resource consumption, and tune its performance based on available VRAM.

**Key Takeaways:**

* RHAIIS deployment is managed with a single, configurable `podman run` command.
* `nvidia-smi` is essential for monitoring VRAM usage.
* The `--gpu-memory-utilization` and `--max-model-len` arguments are your primary tools for memory management.
* Switching between different validated models is as simple as changing the `--model` parameter.

In the next lab, we will build on this foundation to explore multi-GPU deployments and advanced customizations.