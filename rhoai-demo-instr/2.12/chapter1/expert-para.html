<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Expert Parallelism :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="data-para.html">
    <link rel="next" href="start.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mps.html">NVIDIA’s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item is-current-page" data-depth="4">
    <a class="nav-link" href="expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/rhoai_self_managed.html">OpenShift AI Lab</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/oai_install.html">OpenShift AI Install</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/minio-install.html">Lab 1: Foundational Model Serving with Red Hat AI Inference Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/hyperscaler_lab_env.html">Cloud Provider Lab Environments</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/red-hat-docs.html">Red Hat Documentation Reference</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">TL3 Course Edit Guide [Update]</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Section 1</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="intro.html">Serving at Scale</a></li>
    <li><a href="gpu-aas.html">Architecting GPU as a Service</a></li>
    <li><a href="gpu-agg.html">GPU Aggregation</a></li>
    <li><a href="expert-para.html">Expert Parallelism</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Expert Parallelism</h1>
<div class="sect1">
<h2 id="_the_new_frontier_scaling_beyond_density"><a class="anchor" href="#_the_new_frontier_scaling_beyond_density"></a>The New Frontier: Scaling Beyond Density</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We&#8217;ve learned how to scale models by splitting them across GPUs (Tensor) and nodes (Pipeline). But how do models like Mixtral 8x7B or GPT-4 scale to hundreds of billions or even trillions of parameters without requiring an impossibly large amount of computation for every single token?</p>
</div>
<div class="paragraph">
<p>The answer lies in a revolutionary shift in model architecture: the <strong>Mixture of Experts (MoE)</strong>. Expert Parallelism is the specialized strategy designed exclusively to run these highly efficient, sparse models.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_first_understand_the_model_mixture_of_experts_moe"><a class="anchor" href="#_first_understand_the_model_mixture_of_experts_moe"></a>First, Understand the Model: Mixture of Experts (MoE)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before learning the parallelism strategy, you must understand the unique architecture it serves.</p>
</div>
<div class="sect2">
<h3 id="_the_panel_of_specialist_doctors_analogy"><a class="anchor" href="#_the_panel_of_specialist_doctors_analogy"></a>The "Panel of Specialist Doctors" Analogy</h3>
<div class="paragraph">
<p>Imagine a traditional AI model as a single, brilliant general practitioner doctor. This one doctor has to know everything about every possible medical condition. It&#8217;s effective, but not very scalable.</p>
</div>
<div class="paragraph">
<p>An MoE model works like a modern medical clinic:
* <strong>The Patient (A Token):</strong> An input token arrives needing processing.
* <strong>The Triage Nurse (The Router):</strong> A small, fast neural network called a "router" looks at the patient (the token) and decides which specialists are best suited to handle it.
* <strong>The Panel of Specialists (The Experts):</strong> The clinic has many specialist doctors (e.g., a cardiologist, a neurologist, an oncologist). Each is a smaller neural network, highly specialized in one area.
* <strong>Sparse Activation:</strong> The triage nurse doesn&#8217;t send the patient to <strong>every</strong> doctor. It sends them to only the most relevant ones (e.g., the top 2). The other doctors remain idle, saving their energy.</p>
</div>
<div class="paragraph">
<p>This is <strong>sparse activation</strong>. Instead of using the entire massive model for every token, you only use a small fraction of it—the relevant experts. This allows the model to have a huge number of parameters (many specialists) while keeping the actual computation per token incredibly low.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/gpu7.png" alt="MoE Architecture" width="600">
</div>
<div class="title">Figure 1. A visual of the MoE Architecture</div>
</div>
<hr>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_second_understand_the_strategy_expert_parallelism"><a class="anchor" href="#_second_understand_the_strategy_expert_parallelism"></a>Second, Understand the Strategy: Expert Parallelism</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Expert Parallelism aligns the hardware layout with the MoE architecture. It&#8217;s a simple concept: <strong>you put different experts on different GPUs.</strong></p>
</div>
<div class="paragraph">
<p>When the router decides which two experts to use for a specific token, vLLM sends that token <strong>only</strong> to the two GPUs hosting those specific experts. The other GPUs hosting the other experts remain idle for that token, saving power and memory bandwidth.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/gpu5.png" alt="Expert Parallelism Overview" width="600">
</div>
<div class="title">Figure 2. A high-level view of Expert Parallelism</div>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_when_to_use_expert_parallelism"><a class="anchor" href="#_when_to_use_expert_parallelism"></a>When to Use Expert Parallelism</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This is the most straightforward rule of all the parallelism strategies.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Guideline</div>
<div class="paragraph">
<p>Use Expert Parallelism if, and only if, you are deploying a <strong>Mixture of Experts (MoE) model</strong>. This technique is purpose-built for that architecture and is not applicable to standard, dense models.</p>
</div>
</td>
</tr>
</table>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_practical_configuration_with_vllm"><a class="anchor" href="#_practical_configuration_with_vllm"></a>Practical Configuration with vLLM</h2>
<div class="sectionbody">
<div class="paragraph">
<p>vLLM handles the complex routing automatically. You simply need to tell it how your experts are distributed.</p>
</div>
<div class="sect2">
<h3 id="_combining_with_tensor_parallelism"><a class="anchor" href="#_combining_with_tensor_parallelism"></a>Combining with Tensor Parallelism</h3>
<div class="paragraph">
<p>Often, a single "expert" in a large MoE model is itself too large to fit on one GPU. In this case, you combine Expert Parallelism with Tensor Parallelism.</p>
</div>
<div class="listingblock">
<div class="title">Example: Deploying a model with 8 experts, where each expert is large and requires 2 GPUs.</div>
<div class="content">
<pre># Total GPUs needed = 8 experts * 2 GPUs/expert = 16 GPUs

# Distribute the 8 experts across the cluster
expert_parallel_size = 8

# Within each of those expert groups, use 2 GPUs for Tensor Parallelism
tensor_parallel_size = 2</pre>
</div>
</div>
<div class="paragraph">
<p>This configuration tells vLLM:
1.  "I have 8 specialist doctors (experts)."
2.  "Each specialist doctor&#8217;s 'brain' is so large that it needs a team of 2 chefs (GPUs using Tensor Parallelism) to function."</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="data-para.html">Data Parallelism</a></span>
  <span class="next"><a href="start.html">Models with GitOps</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
