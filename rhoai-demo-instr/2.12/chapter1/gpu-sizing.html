<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Sizing for GPUs :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="start.html">
    <link rel="next" href="gpu-scaling.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mps.html">NVIDIAâ€™s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="3">
    <a class="nav-link" href="gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/rhoai_self_managed.html">OpenShift AI Lab</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/oai_install.html">OpenShift AI Install</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/minio-install.html">MinIO S3 Compatible Storage Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/hyperscaler_lab_env.html">Cloud Provider Lab Environments</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/red-hat-docs.html">Red Hat Documentation Reference</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Lab 1: Model Serving with Red Hat AI Inference Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section2.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section3.html">GPU Sharing Technologies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="intro.html">Serving at Scale</a></li>
    <li><a href="start.html">Models with GitOps</a></li>
    <li><a href="gpu-sizing.html">Sizing for GPUs</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Sizing for GPUs</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Successfully deploying a Large Language Model is only half the battle. To build a cost-effective and performant AI platform, you must be able to accurately match the model&#8217;s requirements to the available hardware. Simply throwing oversized, expensive GPUs at every problem is a wasteful and ineffective strategy that leads to high costs and underutilized resources.</p>
</div>
<div class="paragraph">
<p>This module equips you with the fundamental knowledge and tools to analyze an LLM&#8217;s requirements and make informed decisions about the necessary GPU infrastructure. You will learn how to estimate the memory needed for a model and its KV Cache, understand the impact of quantization, and use sizing tools to plan for future deployments.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_story"><a class="anchor" href="#_lab_story"></a>Lab Story</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Platform Engineering team at "InnovateForward Corp" is getting requests to deploy a variety of new models. The business wants to know: "What hardware do we need to run the new 70-billion-parameter model?" and "Can we run this smaller, specialized model on our existing A10G GPUs to save costs?"</p>
</div>
<div class="paragraph">
<p>To answer these questions confidently, the team needs to move beyond guesswork. They must learn how to calculate the vRAM footprint of a given model, including its parameters and the KV Cache. Mastering these calculations will enable them to provide accurate hardware recommendations, optimize resource utilization, and justify infrastructure costs to leadership.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_estimating_model_size"><a class="anchor" href="#_estimating_model_size"></a>Estimating Model Size</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The first step in sizing a GPU is to calculate the amount of video RAM (vRAM) the model&#8217;s weights will consume. The size of the model loaded into memory can be estimated using the following formula:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>M = ((P * 4) / (32 / Q)) * 1.2</pre>
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Symbol</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Description</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>M</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Total GPU memory required for the model weights (in bytes).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>P</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The number of parameters in the model (e.g., 8 billion).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>4b</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The size of a full-precision parameter in bytes (FP32).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Q</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The number of bits used for the model&#8217;s data type after quantization (e.g., 16 for FP16, 8 for INT8).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>1.2</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A multiplier representing a ~20% overhead for loading additional components into GPU memory.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>For example, let&#8217;s calculate the requirement for the <code>ibm-granite/granite-3.3-8b-instruct</code> model, which has 8 billion parameters and uses the FP16 data type (16 bits).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Formula: (((Parameters * 4 bytes) / (32 / Quantization_Bits)) * 1.2) / Bytes_per_GB
(((8 * 4) / (32 / 16)) * 1.2) = 17.9 Gb</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="_quantization"><a class="anchor" href="#_quantization"></a>Quantization</h3>
<div class="paragraph">
<p><strong>Quantization</strong> is a technique used to reduce the size and improve the performance of an LLM by converting its weights to a lower-precision data type. While many models default to FP16 (16 bits), it&#8217;s common to quantize them to INT8 (8 bits) or even INT4 (4 bits) to significantly reduce their memory footprint.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_estimating_kv_cache"><a class="anchor" href="#_estimating_kv_cache"></a>Estimating KV Cache</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Beyond the model&#8217;s weights, the serving runtime (vLLM) also requires vRAM for the <strong>KV Cache</strong>. This cache stores the state of ongoing conversations, and its size is a critical factor in the model&#8217;s ability to handle long contexts.</p>
</div>
<div class="paragraph">
<p>The memory required for the KV Cache is determined by two main factors:
. The memory required per token (this varies by model architecture).
. The maximum context length (number of tokens) you need to support for your use case.</p>
</div>
<div class="paragraph">
<p>For <code>Granite 3.3 8B Instruct</code>, the KV Cache requires about <strong>0.15625 Mb per token</strong>. With a maximum context length of 131,072 tokens, this results in a KV Cache requirement of approximately <strong>20 Gb</strong>.</p>
</div>
<div class="paragraph">
<p>Therefore, to run this model and support its maximum context length, our total estimated vRAM requirement is:
<code>17.9 Gb (Model) + 20 Gb (KV Cache) = ~38 Gb</code></p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Exercise: KV Cache Estimation</strong></p>
</div>
<div class="paragraph">
<p>An NVIDIA A10G or L4 GPU has 24 Gb of vRAM. Given that the Granite 3.3 8B model requires ~17.9 Gb for its weights, what is the maximum context length (in tokens) you could configure for the KV Cache to ensure the entire workload fits on the device?</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_sizing_spreadsheet"><a class="anchor" href="#_sizing_spreadsheet"></a>Sizing Spreadsheet</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To simplify these calculations, Red Hat Services has created a spreadsheet to help with sizing estimates for various LLMs.</p>
</div>
<div class="paragraph">
<p><a href="https://red.ht/llm-sizing" target="_blank" rel="noopener">LLM Sizing and TCO Calculator</a></p>
</div>
<div class="paragraph">
<p>Using the <strong>"Model Sizing"</strong> tab, you can select from a list of popular models to perform a sizing calculation. The spreadsheet provides hardware recommendations based on the model&#8217;s default precision and maximum context length. You can override the context length to see how it impacts the vRAM requirements for your specific use case.</p>
</div>
<div class="paragraph">
<p>The <strong>"Subs &amp; Cost Modeling"</strong> tab can help recommend cloud instance types and provides a high-level Total Cost of Ownership estimate for running the models on OpenShift.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Please keep in mind that all costs are subject to change and are provided as estimates only. They should not be used for official customer quotes.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Exercise: Model Sizing Quantization Comparison</strong></p>
</div>
<div class="paragraph">
<p>Make a copy of the spreadsheet and perform a sizing calculation for the Llama-3-70B model. Compare the vRAM requirements for the un-quantized FP16 version versus the INT4 quantized version provided by Red Hat.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="start.html">Models with GitOps</a></span>
  <span class="next"><a href="gpu-scaling.html">Scaling Architectures</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
