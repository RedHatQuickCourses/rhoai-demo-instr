<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Pipeline Parallelism :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="tensor.html">
    <link rel="next" href="data-para.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mps.html">NVIDIAâ€™s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item is-current-page" data-depth="4">
    <a class="nav-link" href="pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../LABENV/index.html">Lab Environment</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/rhoai_self_managed.html">OpenShift AI Lab</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/oai_install.html">OpenShift AI Install</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/minio-install.html">MinIO S3 Compatible Storage Setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/hyperscaler_lab_env.html">Cloud Provider Lab Environments</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../LABENV/red-hat-docs.html">Red Hat Documentation Reference</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Lab 1: Model Serving with Red Hat AI Inference Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section2.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section3.html">GPU Sharing Technologies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="intro.html">Serving at Scale</a></li>
    <li><a href="gpu-aas.html">Architecting GPU as a Service</a></li>
    <li><a href="gpu-agg.html">GPU Aggregation</a></li>
    <li><a href="pipeline.html">Pipeline Parallelism</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Pipeline Parallelism</h1>
<div class="sect1">
<h2 id="_the_new_problem_the_model_outgrows_the_server"><a class="anchor" href="#_the_new_problem_the_model_outgrows_the_server"></a>The New Problem: The Model Outgrows the Server</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You&#8217;ve mastered Tensor Parallelism, successfully splitting a model across all the GPUs in a single, powerful server. But now you face an even greater challenge: a new model like Llama 3.1 405B has arrived, and its memory requirement is so immense that it <strong>cannot fit even within the combined VRAM of an entire multi-GPU server.</strong></p>
</div>
<div class="paragraph">
<p>When you need to scale beyond the boundaries of a single machine, you need a new strategy: <strong>Pipeline Parallelism</strong>.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_how_it_works_the_factory_assembly_line_analogy"><a class="anchor" href="#_how_it_works_the_factory_assembly_line_analogy"></a>How It Works: The "Factory Assembly Line" Analogy</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While Tensor Parallelism splits a model&#8217;s layers <strong>horizontally</strong> (like a team of chefs working on one dish), Pipeline Parallelism splits the model <strong>vertically</strong>, creating a sequence of stages that operate like a factory assembly line.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>The Assembly Line:</strong> Your model is the product being built.</p>
</li>
<li>
<p><strong>The Workstations:</strong> Each workstation is a separate server (or node) in your cluster.</p>
</li>
<li>
<p><strong>The Process:</strong></p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>The first server (<strong>Workstation 1</strong>) performs the first set of tasks (e.g., layers 1-20 of the model).</p>
</li>
<li>
<p>It then passes its partially finished work (the intermediate activations) to the next server in the line.</p>
</li>
<li>
<p><strong>Workstation 2</strong> performs the next set of tasks (layers 21-40) and passes it on.</p>
</li>
<li>
<p>This continues until the final server completes the final layers and outputs the result.</p>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>To keep all the workstations busy, vLLM sends "micro-batches" of requests down the assembly line one after another, ensuring the entire pipeline stays active and maximizing overall throughput.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/gpu6.png" alt="Pipeline scheduling optimization" width="600">
</div>
<div class="title">Figure 1. A visual example of an Assembly Line (Pipeline)</div>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_the_core_trade_off_throughput_over_latency"><a class="anchor" href="#_the_core_trade_off_throughput_over_latency"></a>The Core Trade-Off: Throughput over Latency</h2>
<div class="sectionbody">
<div class="paragraph">
<p>It is critical to understand that Pipeline and Tensor Parallelism optimize for different goals.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p><strong>Tensor Parallelism REDUCES LATENCY.</strong> By having multiple GPUs work on the same layer simultaneously, it makes individual requests faster.</p>
</li>
<li>
<p><strong>Pipeline Parallelism MAXIMIZES THROUGHPUT.</strong> By keeping multiple servers busy with a continuous flow of micro-batches, it allows you to process more requests over time, but the time for any single request to travel the entire assembly line will be longer.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>A consultant must be able to articulate this trade-off to the customer. Choose Pipeline Parallelism when serving the massive model at all is the priority, and high throughput for many users is more important than the lowest possible latency for a single user.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_the_hardware_advantage_standard_networking_is_enough"><a class="anchor" href="#_the_hardware_advantage_standard_networking_is_enough"></a>The Hardware Advantage: Standard Networking is Enough</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Unlike Tensor Parallelism, which demands high-speed NVLink between GPUs, Pipeline Parallelism has much more relaxed networking requirements.</p>
</div>
<div class="paragraph">
<p>Because data is only passed from one stage to the next in a sequence, the communication overhead is significantly lower. This means Pipeline Parallelism can run efficiently over <strong>standard data center Ethernet</strong>, making it a much more accessible and cost-effective way to scale across multiple machines.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_the_engine_for_multi_node_communication_understanding_ray"><a class="anchor" href="#_the_engine_for_multi_node_communication_understanding_ray"></a>The Engine for Multi-Node Communication: Understanding Ray</h2>
<div class="sectionbody">
<div class="paragraph">
<p>How do different servers in a cluster coordinate this complex assembly line? Python&#8217;s built-in <code>multiprocessing</code> library can only manage processes on a <strong>single</strong> machine. To go across the network, vLLM relies on a distributed computing framework called <strong>Ray</strong>.</p>
</div>
<div class="paragraph">
<p>Ray acts as the coordination layer, or the "factory manager," that:
* Manages resources across multiple nodes.
* Handles the passing of data between the pipeline stages (servers).
* Provides the fault tolerance needed for a distributed system.</p>
</div>
<div class="paragraph">
<p>When you configure vLLM for Pipeline Parallelism, it will automatically use Ray as its execution backend to manage the multi-node workflow.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_when_to_use_pipeline_parallelism"><a class="anchor" href="#_when_to_use_pipeline_parallelism"></a>When to Use Pipeline Parallelism</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The decision framework is a direct extension of our previous lesson.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Guideline</div>
<div class="paragraph">
<p>Use Pipeline Parallelism when your AI model is <strong>so large that it cannot fit within the combined memory of a single multi-GPU server.</strong> It is often used in combination with Tensor Parallelism.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_practical_configuration_with_vllm"><a class="anchor" href="#_practical_configuration_with_vllm"></a>Practical Configuration with vLLM</h3>
<div class="paragraph">
<p>To handle the largest models, you can combine both strategies. vLLM makes this straightforward.</p>
</div>
<div class="listingblock">
<div class="title">Example: Deploying a massive model on a 4-node cluster, with 8 GPUs per node.</div>
<div class="content">
<pre># Total GPUs = 4 nodes * 8 GPUs/node = 32

# Use Tensor Parallelism within each node
tensor_parallel_size = 8

# Use Pipeline Parallelism across the 4 nodes
pipeline_parallel_size = 4</pre>
</div>
</div>
<div class="paragraph">
<p>By setting these two parameters, you instruct vLLM to:
1.  Use <strong>Ray</strong> to create a 4-stage pipeline across the nodes.
2.  Within each stage (on each node), use <strong>Tensor Parallelism</strong> to shard the model layers across that node&#8217;s 8 GPUs.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="tensor.html">Tensor Parallelism</a></span>
  <span class="next"><a href="data-para.html">Data Parallelism</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
