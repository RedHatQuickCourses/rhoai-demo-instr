<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab: Model Serving with Red Hat AI Inference Server :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="vllm-adv.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mps.html">NVIDIAâ€™s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhaiis_overview.html">Red Hat AI Inference Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="gpu-rhaiis.html">GPUs for AI Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a></li>
    <li><a href="rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Lab: Model Serving with Red Hat AI Inference Server</h1>
<div class="sect1">
<h2 id="_1_lab_overview_and_objectives"><a class="anchor" href="#_1_lab_overview_and_objectives"></a>1. Lab Overview and Objectives</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This segment will walk you through the essential first steps of deploying an LLM, verifying its operation, and performing basic performance tuning based on your available hardware resources.</p>
</div>
<div class="paragraph">
<p>You will begin with a pre-configured Red Hat Enterprise Linux (RHEL) environment that already has the necessary NVIDIA drivers installed. Your focus will be entirely on the deployment and management of the RHAIIS container.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_2_deploying_your_first_model"><a class="anchor" href="#_2_deploying_your_first_model"></a>2. Deploying Your First Model</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Our first task is to get a baseline model up and running.</p>
</div>
<div class="paragraph">
<p>It&#8217;s not on Red Hat validated models list, but the ibm-granite/granite-3.3-2b-instruct is small ~3B parameter model that have proven itself time and again for my experiments. So let&#8217;s start with this model and then you tell me what your favorite model is.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a local directory to cache the downloaded model. This saves time on subsequent launches.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">mkdir -p $HOME/rhaiis-cache</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>To make sure everything is working correctly, let&#8217;s run a quick test. This command will start a temporary container and run <code>nvidia-smi</code> inside of it. If it works, you&#8217;ll see details about your GPU.</p>
</div>
<div class="paragraph">
<p>Run this command in <strong>Terminal 1</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo podman run --rm --device nvidia.com/gpu=all \
  docker.io/nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should see an output similar to the one you&#8217;d see if you ran <code>nvidia-smi</code> on the host, showing your GPU model, memory usage, and driver version.</p>
</div>
<div class="sect2">
<h3 id="_3_run_the_rhaiis_container"><a class="anchor" href="#_3_run_the_rhaiis_container"></a>3. Run the RHAIIS Container</h3>
<div class="paragraph">
<p>This is the core of our lab. We&#8217;re going to launch the RHAIIS container image and tell it to serve the <code>ibm-granite/granite-3.3-2b-instruct</code> model.</p>
</div>
<div class="paragraph">
<p>This command might look a bit complex, but let&#8217;s break down the important parts:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>--device nvidia.com/gpu=all</code>: This gives the container access to all available GPUs.</p>
</li>
<li>
<p><code>--security-opt=label=disable</code>: This is important for SELinux systems to allow the container to access local files.</p>
</li>
<li>
<p><code>--shm-size=4GB -p 8000:8000</code>: This allocates shared memory and maps the container&#8217;s port 8000 to the host&#8217;s port 8000, so we can access the API.</p>
</li>
<li>
<p><code>--env "HUGGING_FACE_HUB_TOKEN=$HFTOKEN"</code>: We&#8217;re passing our Hugging Face token into the container.</p>
</li>
<li>
<p><code>-v $HOME/rhaiis-cache:/opt/app-root/src/.cache</code>: This mounts our local cache directory, so the model only needs to be downloaded once.</p>
</li>
<li>
<p><code>registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1</code>: This is the specific RHAIIS container image we&#8217;re using.</p>
</li>
<li>
<p><code>--model ibm-granite/granite-3.3-2b-instruct</code>: This tells the server which model to download and serve.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In <strong>Terminal 1</strong>, copy and paste the following command and press Enter. This will take some time as the model is downloaded and loaded onto the GPU.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct</code></pre>
</div>
</div>
<div class="paragraph">
<p>The server will now start. The first launch will take several minutes as it downloads the model into the cache directory. Subsequent launches will be much faster.</p>
</div>
</div>
<div class="sect2">
<h3 id="_4_verify_the_deployment"><a class="anchor" href="#_4_verify_the_deployment"></a>4. Verify the Deployment</h3>
<div class="paragraph">
<p>Once you see logs indicating "Uvicorn running on <a href="http://0.0.0.0:8000"" class="bare">http://0.0.0.0:8000"</a>, the server is ready.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In <strong> terminal 2</strong> . Do not close the terminal or end the process where the RHAIIS container is running.</p>
</li>
<li>
<p>Use <code>curl</code> to send a test prompt to the server&#8217;s completions endpoint.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What are the key benefits of using Red Hat AI Inference Server?",
  "model": "ibm-granite/granite-3.3-2b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>You should see standard formatted response generated by the model, confirming that the inference server is working correctly.</p>
</div>
<hr>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_4_monitoring_and_tuning_vram_usage"><a class="anchor" href="#_4_monitoring_and_tuning_vram_usage"></a>4. Monitoring and Tuning VRAM Usage</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Understanding and managing GPU memory is the most critical skill for serving LLMs efficiently. Let&#8217;s see how much VRAM our model is using and how to tune it.</p>
</div>
<div class="sect2">
<h3 id="_monitor_gpu_memory"><a class="anchor" href="#_monitor_gpu_memory"></a>Monitor GPU Memory</h3>
<div class="paragraph">
<p>The <code>nvidia-smi</code> command is your primary tool for monitoring the GPU.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In your second terminal, run <code>nvidia-smi</code> in watch mode to see live updates.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">watch -n 1 nvidia-smi</code></pre>
</div>
</div>
</li>
<li>
<p>Observe the <strong>Memory-Usage</strong> column. It will show how much VRAM is being used out of the total available (e.g., <code>8152MiB / 23028MiB</code>). This is the baseline VRAM consumption for this model with default settings.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_tune_for_maximum_context_length"><a class="anchor" href="#_tune_for_maximum_context_length"></a>Tune for Maximum Context Length</h3>
<div class="paragraph">
<p>The <code>--max-model-len</code> argument controls the maximum number of tokens (input prompt + generated output) a request can handle. A larger context length requires more VRAM. Let&#8217;s find the sweet spot for our GPU.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the running RHAIIS container by pressing <code>Ctrl+C</code> in its terminal.</p>
</li>
<li>
<p>Relaunch the server, this time adding the <code>--max-model-len</code> argument. Let&#8217;s start with a value of <code>4096</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --max-model-len 4096 <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Limits the model&#8217;s context length to 4096 tokens.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Once the server is running, check your <code>nvidia-smi</code> watch window. You should see a noticeable decrease in VRAM usage.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_fine_tuning_gpu_memory_utilization"><a class="anchor" href="#_fine_tuning_gpu_memory_utilization"></a>Fine-Tuning GPU Memory Utilization</h3>
<div class="paragraph">
<p>The most direct way to <strong>control the memory vLLM reserves</strong> is with the <code>--gpu-memory-utilization</code> flag. It takes a value between 0.0 and 1.0. The default is <code>0.9</code>, which reserves 90% of the GPU&#8217;s VRAM for this vLLM instance.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the running container with <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Relaunch the server, setting the utilization to 85% to leave more memory for other processes if needed.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --gpu-memory-utilization 0.85 <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Instructs the server to use a maximum of 85% of the available GPU memory.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Observe the change in memory allocation in <code>nvidia-smi</code>. The amount of memory reserved by the server will now be lower. This is a key parameter for running in shared environments.</p>
</li>
</ol>
</div>
<hr>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_5_deploying_an_alternative_model"><a class="anchor" href="#_5_deploying_an_alternative_model"></a>5. Deploying an Alternative Model</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Switching models with RHAIIS is simple. Let&#8217;s deploy the <code>granite-3.1-8b-instruct</code> model.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the current container with <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Run the <code>podman</code> command again, but change the value of the <code>--model</code> argument.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model RedHatAI/granite-3.1-8b-instruct <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>We&#8217;ve switched to the Granite model. The server will download it if it&#8217;s not already in the cache.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Once the server is running, test it with a new <code>curl</code> request. <strong>Remember to update the model name in your request body.</strong></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What is the IBM Granite series of models?",
  "model": "RedHatAI/granite-3.1-8b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>You have now successfully deployed and tested two different validated models, demonstrating the flexibility of the platform.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_6_lab_cleanup"><a class="anchor" href="#_6_lab_cleanup"></a>6. Lab Cleanup</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To stop the services and clean up your environment, simply stop the running container.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the terminal where RHAIIS is running, press <code>Ctrl+C</code>.</p>
</li>
<li>
<p>The <code>--rm</code> flag used in the <code>podman run</code> command ensures the container is automatically removed upon exit.</p>
</li>
</ol>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_7_conclusion"><a class="anchor" href="#_7_conclusion"></a>7. Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this lab, you gained hands-on experience with the core workflow of Red Hat AI Inference Server. You learned how to deploy a model, test its functionality, monitor its resource consumption, and tune its performance based on available VRAM.</p>
</div>
<div class="paragraph">
<p><strong>Key Takeaways:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>RHAIIS deployment is managed with a single, configurable <code>podman run</code> command.</p>
</li>
<li>
<p><code>nvidia-smi</code> is essential for monitoring VRAM usage.</p>
</li>
<li>
<p>The <code>--gpu-memory-utilization</code> and <code>--max-model-len</code> arguments are your primary tools for memory management.</p>
</li>
<li>
<p>Switching between different validated models is as simple as changing the <code>--model</code> parameter.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the next lab, we will build on this foundation to explore multi-GPU deployments and advanced customizations.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="vllm-adv.html">Advanced vLLM Configuration</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
