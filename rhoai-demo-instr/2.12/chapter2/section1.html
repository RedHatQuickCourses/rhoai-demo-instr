<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab 1: Model Serving with Red Hat AI Inference Server :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="section2.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mps.html">NVIDIA’s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="section1.html">Lab 1: Model Serving with Red Hat AI Inference Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section2.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section3.html">GPU Sharing Technologies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="index.html">Red Hat AI Inference Server</a></li>
    <li><a href="section1.html">Lab 1: Model Serving with Red Hat AI Inference Server</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Lab 1: Model Serving with Red Hat AI Inference Server</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>A guided, hands-on exercise for deploying and tuning your first Large Language Model with Red Hat AI Inference Server.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_1_lab_overview_and_objectives"><a class="anchor" href="#_1_lab_overview_and_objectives"></a>1. Lab Overview and Objectives</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to the hands-on lab for Red Hat AI Inference Server (RHAIIS). This guide will walk you through the essential first steps of deploying an LLM, verifying its operation, and performing basic performance tuning based on your available hardware resources.</p>
</div>
<div class="paragraph">
<p>In this lab, you will begin with a pre-configured Red Hat Enterprise Linux (RHEL) environment that already has the necessary NVIDIA drivers installed. Your focus will be entirely on the deployment and management of the RHAIIS container.</p>
</div>
<div class="sect2">
<h3 id="_1_1_learning_objectives"><a class="anchor" href="#_1_1_learning_objectives"></a>1.1. Learning Objectives</h3>
<div class="paragraph">
<p>Upon completing this lab, you will be able to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Deploy RHAIIS with a validated model using a single Podman command.</p>
</li>
<li>
<p>Verify the model is serving correctly by interacting with its API.</p>
</li>
<li>
<p>Monitor the GPU&#8217;s video memory (VRAM) usage in real-time.</p>
</li>
<li>
<p>Tune server parameters to control memory consumption and context length.</p>
</li>
<li>
<p>Deploy and test an alternative model to see the platform&#8217;s flexibility.</p>
</li>
</ul>
</div>
<hr>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_2_prerequisites"><a class="anchor" href="#_2_prerequisites"></a>2. Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Your lab environment has been pre-configured with the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A Red Hat Enterprise Linux 9.x system with a valid subscription.</p>
</li>
<li>
<p>An attached and configured NVIDIA data center GPU with drivers installed.</p>
</li>
<li>
<p>Podman and the NVIDIA Container Toolkit are pre-installed.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You will need to provide:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Credentials for your <strong>Red Hat account</strong> to access <code>registry.redhat.io</code>.</p>
</li>
<li>
<p>A <strong>Hugging Face account</strong> with a User Access Token (read permissions).</p>
</li>
</ul>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_3_deploying_your_first_model"><a class="anchor" href="#_3_deploying_your_first_model"></a>3. Deploying Your First Model</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Our first task is to get a baseline model up and running. We will use <code>RedHatAI/Llama-3.1-8B-Instruct</code>, a capable and widely used model.</p>
</div>
<div class="sect2">
<h3 id="_3_1_authenticate_and_prepare"><a class="anchor" href="#_3_1_authenticate_and_prepare"></a>3.1. Authenticate and Prepare</h3>
<div class="paragraph">
<p>Before running the server, you must log in to the Red Hat registry and configure your Hugging Face token.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Log in to the Red Hat container registry using your account credentials.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman login registry.redhat.io</code></pre>
</div>
</div>
</li>
<li>
<p>Set your Hugging Face token as an environment variable. This allows the server to download the model from the Hugging Face Hub.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># ⚠️ Replace &lt;your_HF_token&gt; with your actual token
export HF_TOKEN="&lt;your_HF_token&gt;"</code></pre>
</div>
</div>
</li>
<li>
<p>Create a local directory to cache the downloaded model. This saves time on subsequent launches.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">mkdir -p $HOME/rhaiis-cache</code></pre>
</div>
</div>
<div class="paragraph">
<p>=== Part 1: Initial System Setup and Prerequisites</p>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Before we can deploy anything, we need to make sure our RHEL system is ready.</p>
</div>
<div class="sect3">
<h4 id="_step_1_install_essential_tools"><a class="anchor" href="#_step_1_install_essential_tools"></a>Step 1: Install Essential Tools</h4>
<div class="paragraph">
<p>Your terminals are already connected to your RHEL server. The first thing we need to do is install some basic utilities that will make our lives easier during this lab. We&#8217;ll install <code>tmux</code>, which lets us keep our terminal sessions running even if we get disconnected.</p>
</div>
<div class="paragraph">
<p>In <strong>Terminal 1</strong> on the right, run the following commands:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Install tmux for persistent sessions
sudo dnf install -y tmux

# Start a new tmux session and attach to it
tmux new-session -d -s rhaiis-lab
tmux attach-session -t rhaiis-lab</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now you&#8217;re inside a <code>tmux</code> session. All the commands we run from now on will stay active in this session. This is great for long-running processes like our inference server.</p>
</div>
</div>
<div class="sect3">
<h4 id="_step_2_install_nvidia_drivers"><a class="anchor" href="#_step_2_install_nvidia_drivers"></a>Step 2: Install NVIDIA Drivers</h4>
<div class="paragraph">
<p>RHAIIS relies on GPU acceleration for its high performance. This means we need to ensure our RHEL system has the correct NVIDIA drivers installed.</p>
</div>
<div class="paragraph">
<p>We&#8217;ll add the necessary software repositories and install the drivers. Continue in <strong>Terminal 1</strong> and run these commands:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Install kernel development tools needed for drivers
sudo dnf install -y kernel-devel-matched kernel-headers

# Add the EPEL repository for extra packages
sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm

# Configure the NVIDIA CUDA repository
sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo

# Install the NVIDIA drivers and kernel module
sudo dnf install -y nvidia-driver-cuda kmod-nvidia-open-dkms

# Reboot the system to load the new drivers
sudo reboot now</code></pre>
</div>
</div>
<div class="paragraph">
<p>The system will now reboot. This is an important step to ensure the kernel recognizes the new drivers. Once the system has restarted, you&#8217;ll need to reconnect to the server via SSH and re-attach to your <code>tmux</code> session.</p>
</div>
</div>
<div class="sect3">
<h4 id="_step_3_container_runtime_setup"><a class="anchor" href="#_step_3_container_runtime_setup"></a>Step 3: Container Runtime Setup</h4>
<div class="paragraph">
<p>Now that the drivers are installed, we need to configure our container runtime, Podman, to be able to use the GPU.</p>
</div>
<div class="paragraph">
<p>First, reconnect to the server and re-attach to your <code>tmux</code> session in <strong>Terminal 1</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Reconnect to the RHEL server
# ssh ec2-user@your_server_ip

# Re-attach to your tmux session
tmux attach-session -t rhaiis-lab</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, let&#8217;s install the NVIDIA Container Toolkit and Podman. This toolkit is what allows containers to see and use the GPU on the host system.</p>
</div>
<div class="paragraph">
<p>Run the following commands in <strong>Terminal 1</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Add the NVIDIA container toolkit repository
curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
  sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

# Install the container toolkit and Podman
sudo dnf install -y nvidia-container-toolkit podman

# Generate the CDI specification for GPU access
sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>To make sure everything is working correctly, let&#8217;s run a quick test. This command will start a temporary container and run <code>nvidia-smi</code> inside of it. If it works, you&#8217;ll see details about your GPU.</p>
</div>
<div class="paragraph">
<p>Run this command in <strong>Terminal 1</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo podman run --rm --device nvidia.com/gpu=all \
  docker.io/nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should see an output similar to the one you&#8217;d see if you ran <code>nvidia-smi</code> on the host, showing your GPU model, memory usage, and driver version.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_3_2_run_the_rhaiis_container"><a class="anchor" href="#_3_2_run_the_rhaiis_container"></a>3.2. Run the RHAIIS Container</h3>
<div class="paragraph">
<p>This is the core of our lab. We&#8217;re going to launch the RHAIIS container image and tell it to serve the <code>ibm-granite/granite-3.3-2b-instruct</code> model.</p>
</div>
<div class="paragraph">
<p>This command might look a bit complex, but let&#8217;s break down the important parts:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>--device nvidia.com/gpu=all</code>: This gives the container access to all available GPUs.</p>
</li>
<li>
<p><code>--security-opt=label=disable</code>: This is important for SELinux systems to allow the container to access local files.</p>
</li>
<li>
<p><code>--shm-size=4GB -p 8000:8000</code>: This allocates shared memory and maps the container&#8217;s port 8000 to the host&#8217;s port 8000, so we can access the API.</p>
</li>
<li>
<p><code>--env "HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN"</code>: We&#8217;re passing our Hugging Face token into the container.</p>
</li>
<li>
<p><code>-v $HOME/rhaiis-cache:/opt/app-root/src/.cache</code>: This mounts our local cache directory, so the model only needs to be downloaded once.</p>
</li>
<li>
<p><code>registry.redhat.io/rhaiis/vllm-rocm-rhel9:3.2.0-1752784646</code>: This is the specific RHAIIS container image we&#8217;re using.</p>
</li>
<li>
<p><code>--model ibm-granite/granite-3.3-2b-instruct</code>: This tells the server which model to download and serve.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In <strong>Terminal 1</strong>, copy and paste the following command and press Enter. This will take some time as the model is downloaded and loaded onto the GPU.
Now, run the server with the <code>podman run</code> command.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model RedHatAI/Llama-3.1-8B-Instruct</code></pre>
</div>
</div>
<div class="paragraph">
<p>The server will now start. The first launch will take several minutes as it downloads the model into the cache directory. Subsequent launches will be much faster.</p>
</div>
</div>
<div class="sect2">
<h3 id="_3_3_verify_the_deployment"><a class="anchor" href="#_3_3_verify_the_deployment"></a>3.3. Verify the Deployment</h3>
<div class="paragraph">
<p>Once you see logs indicating "Uvicorn running on <a href="http://0.0.0.0:8000"" class="bare">http://0.0.0.0:8000"</a>, the server is ready.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open a <strong>new terminal</strong> or SSH session. Do not close the terminal where the container is running.</p>
</li>
<li>
<p>Use <code>curl</code> to send a test prompt to the server&#8217;s completions endpoint.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What are the key benefits of using Red Hat AI Inference Server?",
  "model": "ibm-granite/granite-3.3-2b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>You should see a helpful, well-formatted response generated by the model, confirming that the inference server is working correctly.</p>
</div>
<hr>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_4_monitoring_and_tuning_vram_usage"><a class="anchor" href="#_4_monitoring_and_tuning_vram_usage"></a>4. Monitoring and Tuning VRAM Usage</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Understanding and managing GPU memory is the most critical skill for serving LLMs efficiently. Let&#8217;s see how much VRAM our model is using and how to tune it.</p>
</div>
<div class="sect2">
<h3 id="_4_1_monitor_gpu_memory"><a class="anchor" href="#_4_1_monitor_gpu_memory"></a>4.1. Monitor GPU Memory</h3>
<div class="paragraph">
<p>The <code>nvidia-smi</code> command is your primary tool for monitoring the GPU.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In your second terminal, run <code>nvidia-smi</code> in watch mode to see live updates.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">watch -n 1 nvidia-smi</code></pre>
</div>
</div>
</li>
<li>
<p>Observe the <strong>Memory-Usage</strong> column. It will show how much VRAM is being used out of the total available (e.g., <code>8152MiB / 23028MiB</code>). This is the baseline VRAM consumption for this model with default settings.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_4_2_tune_for_maximum_context_length"><a class="anchor" href="#_4_2_tune_for_maximum_context_length"></a>4.2. Tune for Maximum Context Length</h3>
<div class="paragraph">
<p>The <code>--max-model-len</code> argument controls the maximum number of tokens (input prompt + generated output) a request can handle. A larger context length requires more VRAM. [cite_start]Let&#8217;s find the sweet spot for our GPU. [cite: 135]</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the running container by pressing <code>Ctrl+C</code> in its terminal.</p>
</li>
<li>
<p>Relaunch the server, this time adding the <code>--max-model-len</code> argument. Let&#8217;s start with a value of <code>4096</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --max-model-len 4096 <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Limits the model&#8217;s context length to 4096 tokens.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Once the server is running, check your <code>nvidia-smi</code> watch window. You should see a noticeable increase in VRAM usage.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_4_3_fine_tuning_gpu_memory_utilization"><a class="anchor" href="#_4_3_fine_tuning_gpu_memory_utilization"></a>4.3. Fine-Tuning GPU Memory Utilization</h3>
<div class="paragraph">
<p>The most direct way to control the memory vLLM reserves is with the <code>--gpu-memory-utilization</code> flag. [cite_start]It takes a value between 0.0 and 1.0. [cite: 130] [cite_start]The default is <code>0.9</code>, which reserves 90% of the GPU&#8217;s VRAM. [cite: 603]</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the running container with <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Relaunch the server, setting the utilization to 85% to leave more memory for other processes if needed.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --gpu-memory-utilization 0.85 <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Instructs the server to use a maximum of 85% of the available GPU memory.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Observe the change in memory allocation in <code>nvidia-smi</code>. The amount of memory reserved by the server will now be lower. This is a key parameter for running in shared environments.</p>
</li>
</ol>
</div>
<hr>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_5_deploying_an_alternative_model"><a class="anchor" href="#_5_deploying_an_alternative_model"></a>5. Deploying an Alternative Model</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Switching models with RHAIIS is simple. Let&#8217;s deploy the <code>granite-3.1-8b-instruct</code> model.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the current container with <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Run the <code>podman</code> command again, but change the value of the <code>--model</code> argument.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4g -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache:Z \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model RedHatAI/granite-3.1-8b-instruct <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>We&#8217;ve switched to the Granite model. The server will download it if it&#8217;s not already in the cache.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Once the server is running, test it with a new <code>curl</code> request. <strong>Remember to update the model name in your request body.</strong></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What is the IBM Granite series of models?",
  "model": "RedHatAI/granite-3.1-8b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>You have now successfully deployed and tested two different validated models, demonstrating the flexibility of the platform.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_6_lab_cleanup"><a class="anchor" href="#_6_lab_cleanup"></a>6. Lab Cleanup</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To stop the services and clean up your environment, simply stop the running container.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the terminal where RHAIIS is running, press <code>Ctrl+C</code>.</p>
</li>
<li>
<p>The <code>--rm</code> flag used in the <code>podman run</code> command ensures the container is automatically removed upon exit.</p>
</li>
</ol>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_7_conclusion"><a class="anchor" href="#_7_conclusion"></a>7. Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this lab, you gained hands-on experience with the core workflow of Red Hat AI Inference Server. You learned how to deploy a model, test its functionality, monitor its resource consumption, and tune its performance based on available VRAM.</p>
</div>
<div class="paragraph">
<p><strong>Key Takeaways:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>RHAIIS deployment is managed with a single, configurable <code>podman run</code> command.</p>
</li>
<li>
<p><code>nvidia-smi</code> is essential for monitoring VRAM usage.</p>
</li>
<li>
<p>The <code>--gpu-memory-utilization</code> and <code>--max-model-len</code> arguments are your primary tools for memory management.</p>
</li>
<li>
<p>Switching between different validated models is as simple as changing the <code>--model</code> parameter.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the next lab, we will build on this foundation to explore multi-GPU deployments and advanced customizations.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">Red Hat AI Inference Server</a></span>
  <span class="next"><a href="section2.html">Sizing for GPUs</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
