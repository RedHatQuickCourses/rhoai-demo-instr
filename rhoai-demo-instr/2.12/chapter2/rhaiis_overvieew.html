<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Red Hat AI Inference Server :: OpenShift AI Lab environment instructions</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mps.html">NVIDIAâ€™s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro_rhaiis.html">Sizing for GPUs</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#rhaiis_overview.adoc">rhaiis_overview.adoc</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#gpu_rhaiis.adoc">gpu_rhaiis.adoc</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="rhaiis_overvieew.html">Red Hat AI Inference Server</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Red Hat AI Inference Server</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to the world of <strong>Red Hat AI Inference Server (RHAIIS)</strong>. This hands-on lab is your introduction to a powerful, supported platform for deploying large language models (LLMs) with high performance and flexibility across any hybrid cloud environments. As a key component of the Red Hat AI platform, RHAIIS is available as a standalone product and is included in Red Hat OpenShift AI and Red Hat Enterprise Linux AI (RHEL AI).</p>
</div>
<div class="paragraph">
<p>RHAIIS provides a hardened, supported distribution of the <strong>vLLM</strong> software via prepackaged container. This open-source project is renowned for its high-throughput and memory-efficient performance, achieved through innovative techniques like PagedAttention and continuous batching.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_key_features_of_rhaiis"><a class="anchor" href="#_key_features_of_rhaiis"></a>Key Features of RHAIIS</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Red Hat AI Inference Server leverages several advanced techniques to deliver high-performance LLM serving:</p>
</div>
<div class="sect2">
<h3 id="_vllm_core_and_advanced_parallelism"><a class="anchor" href="#_vllm_core_and_advanced_parallelism"></a>vLLM Core and Advanced Parallelism</h3>
<div class="paragraph">
<p>At its heart, RHAIIS is built on the vLLM serving engine, which maximizes throughput and minimizes latency for LLM inference.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>PagedAttention</strong>: This technique optimizes GPU memory management by virtualizing the key-value (KV) cache for each request. It addresses memory wastage and lowers costs by consuming less memory.</p>
</li>
<li>
<p><strong>Continuous Batching</strong>: It processes requests as they arrive instead of waiting for a full batch to be accumulated, reducing latency and increasing throughput.</p>
</li>
<li>
<p><strong>Tensor Parallelism (TP)</strong>: This feature distributes LLM workloads across multiple GPUs, within a single node, to reduce latency and increase computational throughput. You can configure this using the <code>--tensor-parallel-size</code> argument.</p>
</li>
<li>
<p><strong>Expert Parallelism (EP)</strong>: RHAIIS includes specialized optimizations for efficiently handling Mixture of Experts (MoE) model architectures.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_model_optimization_and_compression"><a class="anchor" href="#_model_optimization_and_compression"></a>Model Optimization and Compression</h3>
<div class="paragraph">
<p>RHAIIS integrates the <strong>LLM Compressor</strong> library, a Developer Preview feature that provides a unified framework for optimizing and compressing large language models before inferencing.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Quantization</strong>: This technique converts model weights and activations to lower-bit formats (such as INT8) to reduce memory usage.</p>
</li>
<li>
<p><strong>Sparsity</strong>: It sets a portion of model weights to zero, enabling more efficient computation.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For RHAIIS 3.2, LLM Compressor is a Developer Preview feature and is not included in the standard container image. The product does, however, support specific quantization variants like FP8 (W8A8) and GGUF, particularly for AMD GPUs.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_deployment_portability_and_multi_accelerator_support"><a class="anchor" href="#_deployment_portability_and_multi_accelerator_support"></a>Deployment Portability and Multi-Accelerator Support</h3>
<div class="paragraph">
<p>RHAIIS is delivered as a container image downloaded from Red Hat Registry, which enables deployment flexibility and  across various connected environments.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Supported Deployment Environments</caption>
<colgroup>
<col style="width: 22.2222%;">
<col style="width: 33.3333%;">
<col style="width: 44.4445%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Environment</th>
<th class="tableblock halign-left valign-top">Supported Versions</th>
<th class="tableblock halign-left valign-top">Deployment Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenShift Container Platform (self-managed)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">4.14-4.19</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deploy on bare-metal hosts or virtual machines.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Red Hat OpenShift Service on AWS (ROSA)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">4.14-4.19</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Requires ROSA STS cluster with GPU-enabled P5 or G5 node types.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Red Hat Enterprise Linux (RHEL)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">9.2-10.0</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deploy on bare-metal hosts or virtual machines.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The server is engineered with robust multi-accelerator support for a diverse range of hardware. RHAIIS 3.2 supports the following data center-grade accelerators:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>NVIDIA GPUs</strong>: A2, A10, A16, A30, A40, A100, L4, L40, L40S, H100, H200, B200, and RTX PRO 6000 Blackwell Server Edition.</p>
</li>
<li>
<p><strong>AMD GPUs</strong>: Instinct M1210 and Instinct M130OX.</p>
</li>
<li>
<p><strong>Google TPUs</strong>: TPU v6e (Developer Preview).</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
