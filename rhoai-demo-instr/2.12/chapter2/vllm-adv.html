<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Advanced vLLM Configuration :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="gpu-rhaiis.html">
    <link rel="next" href="rhaiis_lab.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhaiis_overview.html">Red Hat AI Inference Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="gpu-rhaiis.html">GPUs for AI Inference</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a></li>
    <li><a href="vllm-adv.html">Advanced vLLM Configuration</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Advanced vLLM Configuration</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The vLLM serving runtime provides a robust set of default configurations that work well for many use cases. However, to truly optimize for performance, manage resources in a constrained environment, or enable advanced features like tool calling, you will need to fine-tune the server&#8217;s behavior.</p>
</div>
<div class="paragraph">
<p>This module covers some of the more common and impactful configuration arguments you can set for a vLLM server instance. Understanding these options will allow you to precisely control how the model utilizes GPU memory, handles concurrent requests, and scales across multiple devices, enabling you to tailor the serving environment to your specific application needs.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_setting_arguments"><a class="anchor" href="#_setting_arguments"></a>Setting Arguments</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In OpenShift AI, arguments for a vLLM server can be set directly through the "Model Serving" UI when deploying a single-node instance. Any configuration added through the UI is ultimately represented as a list of strings in the <code>args</code> field of the underlying <code>InferenceService</code> custom resource.</p>
</div>
<div class="paragraph">
<p>For a GitOps-driven approach, you would define these arguments directly in your <code>InferenceService</code> YAML manifest.</p>
</div>
<div class="listingblock">
<div class="title">InferenceService YAML with Custom Arguments</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: my-tuned-model-server
spec:
  predictor:
    model:
      args:
        - '--max-model-len=10000'
        - '--gpu-memory-utilization=0.95'</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>vLLM is a rapidly evolving project. The available options and their behavior can change between versions. When consulting the upstream documentation, always ensure you are viewing the documentation that corresponds to the specific version of vLLM you are using.</p>
</div>
<div class="paragraph">
<p>You can explore all available options for your version by running <code>vllm --help</code> in the container or by referring to the official vLLM documentation: <a href="https://docs.vllm.ai/en/latest/cli/index.html" class="bare">https://docs.vllm.ai/en/latest/cli/index.html</a></p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_common_configuration_options"><a class="anchor" href="#_common_configuration_options"></a>Common Configuration Options</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_model_and_optimization_options"><a class="anchor" href="#_model_and_optimization_options"></a>Model and Optimization Options</h3>
<div class="paragraph">
<p>These arguments control how the model is loaded and how it uses GPU memory, directly impacting the trade-off between performance and context length.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>--max-model-len</code></dt>
<dd>
<p>This is one of the most common arguments to set. It defines the maximum number of tokens (prompt + completion) that the model can handle in a single sequence. You <strong>must</strong> set this to a lower value if you are deploying a model on a GPU that doesn&#8217;t have enough vRAM to support the model&#8217;s default maximum context length.</p>
</dd>
<dt class="hdlist1"><code>--gpu-memory-utilization</code></dt>
<dd>
<p>This determines what percentage of the GPU&#8217;s vRAM is allocated to vLLM. It defaults to <code>0.9</code> (90%). You can cautiously increase this value (e.g., to <code>0.95</code>) to make more memory available for the KV Cache, especially on large GPUs like an H100. However, a small amount of memory must be left free for other CUDA processes.</p>
</dd>
<dt class="hdlist1"><code>--enable-eager-mode</code></dt>
<dd>
<p>This disables the creation of a static CUDA graph, which is normally used to optimize request processing. While this may result in slightly slower response times, it frees up a significant amount of vRAM. This is a useful trade-off when maximizing context length is more important than achieving the absolute lowest latency.</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_concurrency_and_resource_limiting"><a class="anchor" href="#_concurrency_and_resource_limiting"></a>Concurrency and Resource Limiting</h3>
<div class="paragraph">
<p>These options help you manage server load and prevent individual requests from consuming disproportionate resources.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>--max-num-seqs</code></dt>
<dd>
<p>This sets the maximum number of sequences (i.e., concurrent requests) that can be processed at one time. Lowering this value can prevent resource contention and lead to more consistent response times for all users during periods of high traffic.</p>
</dd>
<dt class="hdlist1"><code>--limit-mm-per-prompt</code></dt>
<dd>
<p>For multi-modal models, this option limits the number of multi-modal inputs (e.g., images) that can be included in a single request, preventing resource exhaustion from excessively large inputs.</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_parallelism"><a class="anchor" href="#_parallelism"></a>Parallelism</h3>
<div class="paragraph">
<p>These arguments are essential for deploying models across multiple GPUs or multiple nodes.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>--tensor-parallel-size</code></dt>
<dd>
<p>This specifies the number of GPUs across which the model&#8217;s weights will be sharded using tensor parallelism. This value should typically be equal to the number of GPUs available on a single node.</p>
</dd>
<dt class="hdlist1"><code>--pipeline-parallel-size</code></dt>
<dd>
<p>This is generally used only in multi-node configurations and corresponds to the number of nodes in the deployment. It enables pipeline parallelism, where different layers of the model are processed on different nodes.</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_tool_calling"><a class="anchor" href="#_tool_calling"></a>Tool Calling</h3>
<div class="paragraph">
<p>To enable agentic capabilities, you must configure the vLLM server to correctly handle tool calling.</p>
</div>
<div class="listingblock">
<div class="title">Example Tool Calling Configuration</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">--enable-auto-tool-choice
--tool-call-parser=granite
--chat-template=/app/data/template/tool_chat_template_granite.jinja</code></pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>--enable-auto-tool-choice</code></dt>
<dd>
<p>This is the primary switch to turn on the tool calling functionality.</p>
</dd>
<dt class="hdlist1"><code>--tool-call-parser</code></dt>
<dd>
<p>Specifies the parser required to correctly interpret the tool-calling syntax for a given model family. In this example, we use the <code>granite</code> parser. Other models may require different parsers.</p>
</dd>
<dt class="hdlist1"><code>--chat-template</code></dt>
<dd>
<p>Points to the Jinja chat template that formats the prompt in the specific way the model expects for tool-calling interactions. These templates are typically packaged within the vLLM container image.</p>
</dd>
</dl>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="gpu-rhaiis.html">GPUs for AI Inference</a></span>
  <span class="next"><a href="rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
