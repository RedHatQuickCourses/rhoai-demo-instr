<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>GPUs for AI Inference :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="rhaiis_overview.html">
    <link rel="next" href="vllm-adv.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="3.2">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mps.html">NVIDIAâ€™s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhaiis_overview.html"><strong>Red Hat AI Inference Server (RHAIIS)</strong></a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="gpu-rhaiis.html">GPUs for AI Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Model Serving with Red Hat AI Inference Server (RHAIIS)</span>
    <span class="version">3.2</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">3.2</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a></li>
    <li><a href="intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a></li>
    <li><a href="gpu-rhaiis.html">GPUs for AI Inference</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">GPUs for AI Inference</h1>
<div class="sect1">
<h2 id="introduction"><a class="anchor" href="#introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Successfully deploying a Large Language Model is only half the battle. To build a cost-effective and performant AI platform, you must accurately match the model&#8217;s requirements to the available hardware. Simply throwing oversized, expensive GPUs at every problem is a wasteful and ineffective strategy that leads to high costs and underutilized resources.</p>
</div>
<div class="paragraph">
<p>This module equips you with the fundamental knowledge and tools to analyze an LLM&#8217;s requirements and make informed decisions about the necessary GPU infrastructure. You will learn how to estimate the GPU memory needed for a model including its KV Cache. Understand the impact of quantization, and use sizing tools to plan for future deployments.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/intro_gpu_size.gif" alt=".Objectives" width="600">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="lab-story"><a class="anchor" href="#lab-story"></a>Lab Story</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Platform Engineering team at "InnovateForward Corp" is getting requests to deploy a variety of new models. The business wants to know: "What hardware do we need to run the new 70-billion-parameter model?" and "Can we run this smaller, optimized model on our existing A10G GPUs to save costs?"</p>
</div>
<div class="paragraph">
<p>To answer these questions confidently, the team needs to move beyond guesswork. They must learn how to calculate the video RAM (vRAM) footprint of a given model, including its parameters and the KV Cache. Mastering these calculations will enable them to provide accurate hardware recommendations, optimize resource utilization, and justify infrastructure costs to leadership.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="understanding-vram"><a class="anchor" href="#understanding-vram"></a>The True Cost Blueprint</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_1_model_weights_the_baseline_cost"><a class="anchor" href="#_1_model_weights_the_baseline_cost"></a>1. Model Weights (The Baseline Cost)</h3>
<div class="paragraph">
<p>This is the memory needed to load the model&#8217;s parameters. It&#8217;s a function of the model size (billions of parameters) and the numerical precision used to store each parameter.</p>
</div>
<div class="paragraph">
<p>Calculation:
A formula to estimate the memory for model weights, including overhead:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>M = ((P * 4b) / (32 / Q)) * 1.2</pre>
</div>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Symbol</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Description</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">M</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Total GPU memory required for the model weights (in Gi).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">P</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The number of parameters in the model (e.g., 8 billion).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">4b (bytes)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The size of a full-precision parameter in bytes (FP32).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Q</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The number of bits used for the model&#8217;s precision type after quantization (e.g., 16 for FP16, 8 for INT8).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A multiplier representing a ~20% overhead for loading additional components into GPU memory.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>For example, let&#8217;s calculate the requirement for the <strong>ibm-granite/granite-3.3-8b-instruct model</strong>, which has <strong>8 billion parameters and uses the FP16 data type (16 bits)</strong>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">Formula: (((Parameters * 4 bytes) / (32 / Quantization_Bits)) * 1.2)
Calculation: (((8 * 4) / (32 / 16)) * 1.2) / = ~19 Gi</code></pre>
</div>
</div>
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p><strong>Quantization</strong></p>
</div>
<div class="paragraph">
<p>While not directly part of the sizing calculation, Quantization is a technique used to reduce the size and improve the performance of an LLM by converting its weights to a lower-precision data type. While many models default to FP16 (16 bits), it&#8217;s common to quantize them to INT8 (8 bits) or even INT4 (4 bits) to significantly reduce their memory footprint. <strong>Quantization is the most direct way to reduce the baseline memory cost.</strong></p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Estimated vRAM for Model Weights by Precision</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Model Size (Parameters)</th>
<th class="tableblock halign-left valign-top">FP16 / BF16 (16-bit)</th>
<th class="tableblock halign-left valign-top">INT8 (8-bit)</th>
<th class="tableblock halign-left valign-top">INT4 (4-bit)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">1 Billion</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~2 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~1 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~0.5 GB</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">3 Billion</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~6 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~3 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~1.5 GB</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">8 Billion</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~16 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~8 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~4 GB</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">13 Billion</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~26 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~13 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~6.5 GB</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">34 Billion</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~68 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~34 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~17 GB</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">70 Billion</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~140 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~70 GB</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">~35 GB</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_2_the_kv_cache_context_memory"><a class="anchor" href="#_2_the_kv_cache_context_memory"></a>2. The KV Cache (Context Memory)</h3>
<div class="paragraph">
<p>The Key-Value (KV) Cache stores attention data for the sequence being processed. For modern LLMs, this is often the largest and most volatile consumer of VRAM. Its size is not fixed; it grows dynamically based on your workload.</p>
</div>
<div class="paragraph">
<p>Key Drivers: The size of the KV Cache is directly proportional to:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Batch Size: The number of requests you process concurrently.</p>
</li>
<li>
<p>Context Length: The number of tokens (input + output) in each request.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Impact: For applications with long context windows (e.g., document summarization) or high batch sizes, the KV Cache can easily consume more VRAM than the model weights themselves.</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p>For <strong>ibm-granite/granite-3.3-8b-instruct model</strong>, the KV Cache requires about 0.15625 MB per token. With a maximum context length of 131,072 tokens, this results in a KV Cache requirement of approximately 20 Gi.</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_3_cuda_system_overhead"><a class="anchor" href="#_3_cuda_system_overhead"></a>3. CUDA &amp; System Overhead</h3>
<div class="paragraph">
<p>This is the fixed cost of doing business on a GPU. It includes memory consumed by the NVIDIA CUDA kernels, the core PyTorch and vLLM libraries, and various system buffers required to manage the computation.</p>
</div>
<div class="paragraph">
<p>Estimated Cost: Budget an additional 10-20% of the model&#8217;s weight VRAM for this overhead (as included in our formula).</p>
</div>
</div>
<div class="sect2">
<h3 id="_4_model_activations"><a class="anchor" href="#_4_model_activations"></a>4. Model Activations</h3>
<div class="paragraph">
<p>These are the intermediate values calculated during the model&#8217;s forward pass. While their memory impact is far smaller than the KV Cache, they are a non-zero factor.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="real-world-equation"><a class="anchor" href="#real-world-equation"></a>The Real-World vRAM Equation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A practical formula for estimating your total memory requirement looks like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-text hljs" data-lang="text">Total vRAM Needed â‰ˆ (VRAM for Model Weights &amp; Overhead) + (VRAM for Max KV Cache)
Using our 8B model example:
19.0 Gi (Model &amp; Overhead) + 20 Gi (KV Cache) = ~39 Gi</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Don&#8217;t Be Fooled by the "Sticker Price"</div>
A model&#8217;s advertised size is not its final cost in production. A 13B parameter model might list a ~26 GB requirement for its FP16 weights, suggesting it could fit on a 32 GB GPU. However, with a large batch size and long context window for the KV Cache, the actual VRAM requirement can easily exceed 40 GB.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Golden Rule: Always profile your specific use case with realistic batch sizes and context lengths. Never select hardware based solely on the VRAM needed for model weights.</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p><strong>Exercise: KV Cache Estimation</strong></p>
</div>
<div class="paragraph">
<p>An NVIDIA A10G or L4 GPU has 24 GB of vRAM. Given that the <strong>ibm-granite/granite-3.3-8b-instruct model</strong> requires ~19.0 Gi for its weights and overhead, what is the maximum context length (in tokens) you could configure for the KV Cache to ensure the entire workload fits on the device? ( using 0.15625 MB per token, 95% GPU usage )</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="tensor-parallelism"><a class="anchor" href="#tensor-parallelism"></a>When Your Model is Too Big for One GPU</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This is where GPU aggregation strategies become essential. The first and most common strategy for this scenario is Tensor Parallelism.</p>
</div>
<div class="sect2">
<h3 id="_tensor_parallelism"><a class="anchor" href="#_tensor_parallelism"></a>Tensor Parallelism</h3>
<div class="paragraph">
<p>Tensor Parallelism solves the memory problem by splitting a model&#8217;s layers horizontally across multiple GPUs within a single server.</p>
</div>
<div class="paragraph">
<p>In technical terms, each GPU holds a "shard" or a slice of the model&#8217;s weight matrices. They process their portion of the data in parallel and then use a high-speed interconnect to exchange the results, a process known as an all-reduce operation.</p>
</div>
</div>
<div class="sect2">
<h3 id="_when_to_use_tensor_parallelism"><a class="anchor" href="#_when_to_use_tensor_parallelism"></a>When to Use Tensor Parallelism</h3>
<div class="paragraph">
<p>The rule for using Tensor Parallelism is simple.</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p>Use Tensor Parallelism when your AI model is too large to fit on a single GPU, but it can fit within the combined memory of all GPUs in a single server.
It is the ideal solution for scaling up within the bounds of a single multi-GPU node.</p>
</div>
</div>
</div>
<hr>
</div>
<div class="sect2">
<h3 id="_vllm_configuration"><a class="anchor" href="#_vllm_configuration"></a>vLLM Configuration</h3>
<div class="paragraph">
<p>vLLM makes it simple to enable Tensor Parallelism. You just need to specify how many GPUs you want to use.</p>
</div>
<div class="sect3">
<h4 id="_example_deploying_llama_3_70b_on_a_server_with_2_gpus"><a class="anchor" href="#_example_deploying_llama_3_70b_on_a_server_with_2_gpus"></a>Example: Deploying Llama 3 70B on a server with 2 GPUs</h4>
<div class="paragraph">
<p>A 70-billion parameter model like Llama 3 requires ~140GB of memory for its weights, which will not fit on a single 80GB H100 GPU. However, it will fit across two. You would configure vLLM with tensor_parallel_size=2.</p>
</div>
</div>
<div class="sect3">
<h4 id="_command_line_configuration"><a class="anchor" href="#_command_line_configuration"></a>Command-Line Configuration</h4>
<div class="paragraph">
<p>When launching vLLM from the command line, use the --tensor-parallel-size argument.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">Launch vLLM using 2 GPUs for Tensor Parallelism
python -m vllm.entrypoints.api_server
--model "meta-llama/Llama-3-70B-Instruct"
--tensor-parallel-size 2</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_python_code_configuration"><a class="anchor" href="#_python_code_configuration"></a>Python Code Configuration</h4>
<div class="paragraph">
<p>When using vLLM within your Python application, set the tensor_parallel_size in the EngineArgs.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from vllm import EngineArgs, LLMEngine
engine_args = EngineArgs( model="meta-llama/Llama-3-70B-Instruct", tensor_parallel_size=2 )</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="rhaiis_overview.html"><strong>Red Hat AI Inference Server (RHAIIS)</strong></a></span>
  <span class="next"><a href="vllm-adv.html">Advanced vLLM Configuration</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
