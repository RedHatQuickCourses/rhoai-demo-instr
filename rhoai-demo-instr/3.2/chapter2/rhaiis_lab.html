<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab: Model Serving with Red Hat AI Inference Server :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="vllm-adv.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="3.2">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mps.html">NVIDIAâ€™s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhaiis_overview.html"><strong>Red Hat AI Inference Server (RHAIIS)</strong></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="gpu-rhaiis.html">GPUs for AI Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Model Serving with Red Hat AI Inference Server (RHAIIS)</span>
    <span class="version">3.2</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">3.2</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a></li>
    <li><a href="intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a></li>
    <li><a href="rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Lab: Model Serving with Red Hat AI Inference Server</h1>
<div class="sect1">
<h2 id="_lab_overview_and_objectives"><a class="anchor" href="#_lab_overview_and_objectives"></a>Lab Overview and Objectives</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This segment will walk you through the essential first steps of deploying an LLM, verifying its operation, and performing basic performance tuning based on your available hardware resources.</p>
</div>
<div class="paragraph">
<p>You will begin with a pre-configured Red Hat Enterprise Linux (RHEL) environment that already has the necessary NVIDIA drivers installed. Your focus will be entirely on the deployment and management of the RHAIIS container.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_stop_the_existing_ai_model"><a class="anchor" href="#_stop_the_existing_ai_model"></a>Stop the Existing AI Model.</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Our first task is to get our own baseline model up and running.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>We first need to stop the running lab model to free up the GPU. In the top terminal run:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo systemctl stop rhaiis.service</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>To make sure everything is working correctly, let&#8217;s run a quick test. This command will start a temporary container and run <code>nvidia-smi</code> inside of it. If it works, you&#8217;ll see details about your GPU. Keep this handy, it&#8217;s great way to test container access to GPUs across environments.</p>
</div>
<div class="paragraph">
<p>Run this command in <strong>Terminal 1</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo podman run --rm --device nvidia.com/gpu=all \
  docker.io/nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi</code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/nvidia_smi.png" alt="Nvidia GPU Status">
</div>
<div class="title">Figure 1. Nvidia GPU Status</div>
</div>
<div class="paragraph">
<p>You should see an output similar to the one you&#8217;d see if you ran <code>nvidia-smi</code> on the host, showing your GPU model, memory usage, and driver version.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deploying_your_first_model"><a class="anchor" href="#_deploying_your_first_model"></a>Deploying Your First Model</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_1_create_a_cache_directory"><a class="anchor" href="#_1_create_a_cache_directory"></a>1. Create a Cache Directory</h3>
<div class="paragraph">
<p>Create a local directory to cache the downloaded model. This saves time on subsequent launches.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">mkdir -p $HOME/rhaiis-cache</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_2_run_the_rhaiis_container"><a class="anchor" href="#_2_run_the_rhaiis_container"></a>2. Run the RHAIIS Container</h3>
<div class="paragraph">
<p>This is the core of our lab. We&#8217;re going to launch the RHAIIS container image and tell it to serve the <code>ibm-granite/granite-3.3-2b-instruct</code> model.</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>This command might look a bit complex, but let&#8217;s break down the important parts:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong><code>--device nvidia.com/gpu=all</code>:</strong> This gives the container access to all available GPUs.</p>
</li>
<li>
<p><strong><code>--security-opt=label=disable</code>:</strong> This is important for SELinux systems to allow the container to access local files.</p>
</li>
<li>
<p><strong><code>--shm-size=4GB -p 8000:8000</code>:</strong> This allocates shared memory and maps the container&#8217;s port 8000 to the host&#8217;s port 8000, so we can access the API.</p>
</li>
<li>
<p><strong><code>--env "HUGGING_FACE_HUB_TOKEN=$HFTOKEN"</code>:</strong> We&#8217;re passing our Hugging Face token into the container.</p>
</li>
<li>
<p><strong><code>-v $HOME/rhaiis-cache:/opt/app-root/src/.cache</code>:</strong> This mounts our local cache directory, so the model only needs to be downloaded once.</p>
</li>
<li>
<p><strong><code>registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1</code>:</strong> This is the specific RHAIIS container image we&#8217;re using.</p>
</li>
<li>
<p><strong><code>--model ibm-granite/granite-3.3-2b-instruct</code>:</strong> This tells the server which model to download and serve.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>In <strong>Terminal 1</strong>, copy and paste the following command and press Enter. This will take some time as the model is downloaded and loaded onto the GPU.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4GB -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  --env "HF_HUB_OFFLINE=0" \
  --env=VLLM_NO_USAGE_STATS=1 \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_3_monitor_gpu_memory"><a class="anchor" href="#_3_monitor_gpu_memory"></a>3. Monitor GPU Memory</h3>
<div class="paragraph">
<p>There are a few tools for monitoring GPU memory. Since we are using Nvidia GPUs, where using their tools.</p>
</div>
<div class="paragraph">
<p>The <code>nvidia-smi</code> command is your primary tool for monitoring the GPU. We saw this demonstrated in our quick GPU test earlier. For this part of the lab, let&#8217;s use the <code>nvtop</code> command.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In your second terminal, run <code>nvtop</code>  to see live updates.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">nvtop</code></pre>
</div>
</div>
</li>
<li>
<p>Keep an eye on the Memory-Usage box. It will show you how much of the total available VRAM is being used as the model loads its weights and initializes the KV Cache, eventually consuming most of the GPU&#8217;s memory. This initial usage represents the baseline VRAM consumption for the model with default settings.</p>
</li>
</ol>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/nvtop.png" alt="Nvidia GPU Status" width="480">
</div>
<div class="title">Figure 2. nvtop command status</div>
</div>
</div>
<div class="sect2">
<h3 id="_4_verify_the_deployment"><a class="anchor" href="#_4_verify_the_deployment"></a>4. Verify the Deployment</h3>
<div class="paragraph">
<p>Once you see logs indicating "gRPC Server started on <a href="http://0.0.0.0:8033"" class="bare">http://0.0.0.0:8033"</a>, the server is ready.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In <strong> terminal 2</strong>, stop the <code>nvtop</code> process by pressing CTRL-C.</p>
<div class="ulist">
<ul>
<li>
<p>Do not close the terminal or end the process where the RHAIIS container is running.*</p>
</li>
</ul>
</div>
</li>
<li>
<p>In <strong> terminal 2</strong>, Use <code>curl</code> to send a test prompt to the server&#8217;s completions endpoint.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What are the key benefits of using Red Hat AI Inference Server?",
  "model": "ibm-granite/granite-3.3-2b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>You should now see a standard, formatted response from the model, which confirms that the inference server is working correctly.</p>
</div>
<div class="paragraph">
<p>Feel free to change the prompt and try a few different queries to experiment with the model.</p>
</div>
<hr>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_4_monitoring_and_tuning_vram_usage"><a class="anchor" href="#_4_monitoring_and_tuning_vram_usage"></a>4. Monitoring and Tuning VRAM Usage</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Understanding and managing GPU memory is the most critical skill for serving LLMs efficiently. Now that we validated it&#8217;s working . Let&#8217;s see how much VRAM our model is using and how to tune it.</p>
</div>
<div class="sect2">
<h3 id="_monitor_gpu_memory"><a class="anchor" href="#_monitor_gpu_memory"></a>Monitor GPU Memory</h3>
<div class="paragraph">
<p>This time let&#8217;s use the  <code>nvidia-smi</code> command is your primary tool for monitoring the GPU.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In your second terminal, run <code>nvidia-smi</code> in watch mode to see live updates.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">watch -n 1 nvidia-smi</code></pre>
</div>
</div>
</li>
<li>
<p>Observe the <strong>Memory-Usage</strong> column. It will show how much VRAM is being used out of the total available (e.g., <code>20052MiB / 23028MiB</code>). This is the baseline VRAM consumption for this model with default settings.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Press Ctrl-C to stop exist the nvidia smi command.</p>
</div>
<div class="paragraph">
<p>The other command available in this lab is the <code>nvtop</code> command. I like this command better because of the graph style concumption metrics provided for GPU memory usage and GPU processing usage.</p>
</div>
</div>
<div class="sect2">
<h3 id="_tune_for_maximum_context_length"><a class="anchor" href="#_tune_for_maximum_context_length"></a>Tune for Maximum Context Length</h3>
<div class="paragraph">
<p>The <code>--max-model-len</code> argument controls the maximum number of tokens (input prompt + generated output) a request can handle. A larger context length requires more VRAM. Let&#8217;s find the sweet spot for our GPU.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the running RHAIIS container by pressing <code>Ctrl+C</code> in its terminal.</p>
</li>
<li>
<p>Relaunch the server, this time adding the <code>--max-model-len</code> argument. Let&#8217;s start with a value of <code>80000</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4GB -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  --env "HF_HUB_OFFLINE=0" \
  --env=VLLM_NO_USAGE_STATS=1 \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --max-model-len 80000 <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Limits the model&#8217;s context length to 80000 tokens from the max 128,000 tokens.</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Once the server is running, check your <code>nvidia-smi</code>, nvtop watch windows. You should see a noticeable decrease in VRAM usage.</p>
</div>
<div class="paragraph">
<p>In reality,  what happens in this case is that the inference engine does limit the models max context lgenth to 80K, however, the RHAIIS (vLLM) engine still consumed all the available GPU memory it could based on the .90 (90%) utilization default setting.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s reduce this memory setting next to see how this effects our GPU memory consumption.</p>
</div>
</div>
<div class="sect2">
<h3 id="_fine_tuning_gpu_memory_utilization"><a class="anchor" href="#_fine_tuning_gpu_memory_utilization"></a>Fine-Tuning GPU Memory Utilization</h3>
<div class="paragraph">
<p>The most direct way to <strong>control the memory vLLM reserves</strong> is with the <code>--gpu-memory-utilization</code> flag. It takes a value between 0.0 and 1.0. The default is <code>0.9</code>, which reserves 90% of the GPU&#8217;s VRAM for this vLLM instance.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the running container with <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Relaunch the server, setting the utilization to 70% to leave more memory for other processes if needed.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4GB -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  --env "HF_HUB_OFFLINE=0" \
  --env=VLLM_NO_USAGE_STATS=1 \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --gpu-memory-utilization 0.70 <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Instructs the server to use a maximum of 70% of the available GPU memory.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Observe the change in memory allocation in <code>nvidia-smi</code>. The amount of memory reserved by the server will now be lower. This is a key parameter for running in shared environments.</p>
</li>
</ol>
</div>
<hr>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_5_deploying_an_alternative_model"><a class="anchor" href="#_5_deploying_an_alternative_model"></a>5. Deploying an Alternative Model</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Switching models with RHAIIS is simple. Let&#8217;s deploy the <code>granite-3.1-8b-instruct</code> model.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the current container with <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Run the <code>podman</code> command again, but change the value of the <code>--model</code> argument.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">podman run --rm -it --name rhaiis-server \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --shm-size=4GB -p 8000:8000 \
  --env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
  --env "HF_HUB_OFFLINE=0" \
  --env=VLLM_NO_USAGE_STATS=1 \
  -v $HOME/rhaiis-cache:/opt/app-root/src/.cache \
  registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1754088865-hotfix-1 \
  --model RedHatAI/granite-3.1-8b-instruct <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>We&#8217;ve switched to the Granite 3.1- 8b model. The server will download it if it&#8217;s not already in the cache.</td>
</tr>
</table>
</div>
</li>
<li>
<p>Once the server is running, test it with a new <code>curl</code> request. <strong>Remember to update the model name in your request body.</strong></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
  "prompt": "What is the IBM Granite series of models?",
  "model": "RedHatAI/granite-3.1-8b-instruct",
  "max_tokens": 150
}' | jq .choices[0].text</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>You have now successfully deployed and tested two different validated models, demonstrating the flexibility of the platform.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_6_trouble_shooting_the_larger_model"><a class="anchor" href="#_6_trouble_shooting_the_larger_model"></a>6. Trouble Shooting the Larger Model</h2>
<div class="sectionbody">
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>If you deployed your lab using the suggested environment, it&#8217;s highly likely the larger Granite-8B model just crashed during loading. If you were monitoring with nvtop, you would have seen the GPU usage drop to zero.</p>
</div>
<div class="paragraph">
<p>This was expected. Earlier in the lab guide, you were asked to calculate the context length for the Granite 8B model. Let&#8217;s revisit that information&#8230;&#8203;</p>
</div>
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p><strong>Exercise: KV Cache Estimation</strong></p>
</div>
<div class="paragraph">
<p>An NVIDIA A10G or L4 GPU has 24 GB of vRAM. Given that the <strong>RedHatAI/granite-3.1-8b-instruct</strong> requires ~19.0 Gi for its weights and overhead, what is the maximum context length (in tokens) you could configure for the KV Cache to ensure the entire workload fits on the device? ( using 0.15625 MB per token, 95% GPU usage )</p>
</div>
</div>
</div>
<div class="paragraph">
<p><strong>You will need to apply this answer as a argument / setting in order to launch the Granite-3.1-8B model successfully.</strong></p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Start small and during a successful launch, the engine will display the max token config for the available memory.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_7_wrapping_up_this_experience"><a class="anchor" href="#_7_wrapping_up_this_experience"></a>7. Wrapping up this Experience</h2>
<div class="sectionbody">
<div class="paragraph">
<p>I know, I know you left you hanging on this last lab exercise, we&#8217;ll add an arcade that shows the answer soon, until then everything you need is in this course.</p>
</div>
<div class="paragraph">
<p>The key points in this lab were:</p>
</div>
<div class="paragraph">
<p>Gain hands-on experience with the deployment of Red Hat AI Inference Server.</p>
</div>
<div class="paragraph">
<p>You learned how to calculate a model GPU requirements, deploy a model, test its functionality, monitor its resource consumption, and tune its performance based on available VRAM.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_8_lab_cleanup"><a class="anchor" href="#_8_lab_cleanup"></a>8. Lab Cleanup</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To stop the services and clean up your environment, simply stop the running container.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the terminal where RHAIIS is running, press <code>Ctrl+C</code>.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>The <code>--rm</code> flag used in the <code>podman run</code> command ensures the container is automatically removed upon exit.</p>
</li>
</ol>
</div>
</li>
<li>
<p>In the terminal where nvtop nvidia-smi is running, press <code>Ctrl+C</code>.
.Delete the services on the Demo Platform.</p>
</li>
</ol>
</div>
<hr>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="vllm-adv.html">Advanced vLLM Configuration</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
