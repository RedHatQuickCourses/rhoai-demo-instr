<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Tensor Parallelism :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="gpu-agg.html">
    <link rel="next" href="pipeline.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mps.html">NVIDIAâ€™s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="4">
    <a class="nav-link" href="tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/intro_rhaiis.html">Sizing for GPUs</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#rhaiis_overview.adoc">rhaiis_overview.adoc</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#gpu_rhaiis.adoc">gpu_rhaiis.adoc</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="intro.html">Serving at Scale</a></li>
    <li><a href="gpu-aas.html">Architecting GPU as a Service</a></li>
    <li><a href="gpu-agg.html">GPU Aggregation</a></li>
    <li><a href="tensor.html">Tensor Parallelism</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Tensor Parallelism</h1>
<div class="sect1">
<h2 id="_the_problem_your_model_is_too_big_for_one_gpu"><a class="anchor" href="#_the_problem_your_model_is_too_big_for_one_gpu"></a>The Problem: Your Model is Too Big for One GPU</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You&#8217;ve been asked to deploy a powerful new Large Language Model, but when you try to load it, you hit the most common roadblock in AI infrastructure: the <code>CUDA out of memory</code> error. The model&#8217;s weights and its runtime KV cache are simply too large to fit into the memory of a single GPU, even a top-tier one like an A100 or H100.</p>
</div>
<div class="paragraph">
<p>This is where GPU aggregation strategies become essential. The first and most common strategy for this scenario is <strong>Tensor Parallelism</strong>.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_how_it_works_the_team_of_chefs_analogy"><a class="anchor" href="#_how_it_works_the_team_of_chefs_analogy"></a>How It Works: The "Team of Chefs" Analogy</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Tensor Parallelism solves the memory problem by splitting a model&#8217;s layers <strong>horizontally</strong> across multiple GPUs within a single server.</p>
</div>
<div class="paragraph">
<p>Imagine a team of chefs (your GPUs) tasked with preparing an incredibly complex recipe (a single layer of the AI model).</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Instead of one chef doing all the work</strong>, the recipe is split. Chef 1 handles the vegetables, Chef 2 handles the sauces, and so on.</p>
</li>
<li>
<p><strong>They work simultaneously</strong>, which dramatically speeds up the preparation time. This is how Tensor Parallelism reduces latency.</p>
</li>
<li>
<p><strong>At the end</strong>, they must quickly combine their finished components to create the final dish. This requires constant, high-speed communication between the chefs.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In technical terms, each GPU holds a "shard" or a slice of the model&#8217;s weight matrices. They process their portion of the data in parallel and then use a high-speed interconnect to exchange the results, a process known as an <code>all-reduce</code> operation.</p>
</div>
<div class="imageblock unresolved">
<div class="content">
<img src="tensor-parallelism-overview.png" alt="Tensor Parallelism" width="600">
</div>
<div class="title">Figure 1. A high-level view of Tensor Parallelism</div>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_the_critical_prerequisite_high_speed_interconnect"><a class="anchor" href="#_the_critical_prerequisite_high_speed_interconnect"></a>The Critical Prerequisite: High-Speed Interconnect</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The "communication between chefs" is the most critical part of this process. The performance of Tensor Parallelism is fundamentally dependent on the bandwidth of the connection between the GPUs.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Tensor Parallelism is designed to be used with high-speed, direct interconnects like <strong>NVIDIA NVLink</strong> or NVSwitch. Using it over a slower interconnect like standard PCIe will create a severe communication bottleneck, negating the performance benefits and potentially leading to slower results than using a single GPU.</p>
</div>
<div class="paragraph">
<p><strong>A consultant&#8217;s key takeaway:</strong> When designing a server for Tensor Parallelism, NVLink is not just a nice-to-have; it is a core requirement.</p>
</div>
</td>
</tr>
</table>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_when_to_use_tensor_parallelism"><a class="anchor" href="#_when_to_use_tensor_parallelism"></a>When to Use Tensor Parallelism</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The rule for using Tensor Parallelism is simple and prescriptive.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Guideline</div>
<div class="paragraph">
<p>Use Tensor Parallelism when your AI model is <strong>too large to fit on a single GPU</strong>, but it <strong>can fit within the combined memory of all GPUs in a single server</strong>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>It is the ideal solution for scaling up within the bounds of a single, powerful, multi-GPU node.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_practical_configuration_with_vllm"><a class="anchor" href="#_practical_configuration_with_vllm"></a>Practical Configuration with vLLM</h2>
<div class="sectionbody">
<div class="paragraph">
<p>vLLM makes it simple to enable Tensor Parallelism. You just need to specify how many GPUs you want to use.</p>
</div>
<div class="paragraph">
<div class="title">Example: Deploying Llama 3 70B on a server with 2 GPUs</div>
<p>A 70-billion parameter model like Llama 3 requires ~140GB of memory, which will not fit on a single 80GB H100 GPU. However, it will fit across two.</p>
</div>
<div class="paragraph">
<p>You would configure vLLM with <code>tensor_parallel_size=2</code>.</p>
</div>
<div class="sect2">
<h3 id="_command_line_configuration"><a class="anchor" href="#_command_line_configuration"></a>Command-Line Configuration</h3>
<div class="paragraph">
<p>When launching vLLM from the command line, use the <code>--tensor-parallel-size</code> argument.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Launch vLLM using 2 GPUs for Tensor Parallelism
python -m vllm.entrypoints.api_server \
    --model "meta-llama/Llama-3-70B-Instruct" \
    --tensor-parallel-size 2

=== Python Code Configuration

When using vLLM within your Python application, set the tensor_parallel_size in the EngineArgs.

[]
from vllm import EngineArgs, LLMEngine

engine_args = EngineArgs(
    model="meta-llama/Llama-3-70B-Instruct",
    tensor_parallel_size=2
)

llm_engine = LLMEngine.from_engine_args(engine_args)</code></pre>
</div>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="gpu-agg.html">GPU Aggregation</a></span>
  <span class="next"><a href="pipeline.html">Pipeline Parallelism</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
