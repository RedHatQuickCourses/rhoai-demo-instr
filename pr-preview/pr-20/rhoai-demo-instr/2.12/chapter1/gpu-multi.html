<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab: Multi-GPU Model Deployment :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="vllm-adv.html">
    <link rel="next" href="gpu-nodes.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mps.html">NVIDIAâ€™s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item is-current-page" data-depth="3">
    <a class="nav-link" href="gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/intro_rhaiis.html">Sizing for GPUs</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#rhaiis_overview.adoc">rhaiis_overview.adoc</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#gpu_rhaiis.adoc">gpu_rhaiis.adoc</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="intro.html">Serving at Scale</a></li>
    <li><a href="start.html">Models with GitOps</a></li>
    <li><a href="gpu-multi.html">Lab: Multi-GPU Model Deployment</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Lab: Multi-GPU Model Deployment</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Deploying Large Language Models often pushes the boundaries of a single GPU&#8217;s memory. In this lab, you will learn the practical steps to deploy an LLM that requires more vRAM than one accelerator can provide by using <strong>tensor parallelism</strong>. This technique shards the model&#8217;s weights across multiple GPUs on a single node, allowing you to run larger, more powerful models.</p>
</div>
<div class="paragraph">
<p>You will walk through a realistic scenario: first attempting to deploy a model on a single GPU, observing the predictable memory failure, and then successfully redeploying it across two GPUs. This process will solidify your understanding of resource requirements and multi-GPU configurations in OpenShift AI.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_story"><a class="anchor" href="#_lab_story"></a>Lab Story</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Platform Engineering team at "InnovateForward Corp" has a new task: deploy the <code>ibm-granite/granite-3.3-8b-instruct</code> model for an internal developer team. Based on their sizing calculations from the previous module, they know that the model and its full KV Cache require nearly 38Gb of vRAM, which is more than a single 24Gb GPU can handle.</p>
</div>
<div class="paragraph">
<p>To demonstrate the importance of correct sizing, they decide to first attempt the deployment on a single GPU, fully expecting it to fail. They will then use this failure as a learning opportunity to correctly reconfigure the deployment, using two GPUs and enabling tensor parallelism to bring the model online successfully.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_1_initial_deployment_attempt_1_gpu"><a class="anchor" href="#_step_1_initial_deployment_attempt_1_gpu"></a>Step 1: Initial Deployment Attempt (1 GPU)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First, we will deploy the model using the OpenShift AI dashboard and request a single GPU.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>From the OpenShift AI Dashboard, select the <strong>Serving at Scale</strong> project.</p>
</li>
<li>
<p>Navigate to the <strong>Model Serving</strong> tab and click <strong>Deploy model</strong>.</p>
</li>
<li>
<p>Select the <strong>vLLM</strong> Serving Runtime and fill in the deployment details with the following information:</p>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><strong>Model deployment name:</strong> <code>granite-8b</code></p>
</li>
<li>
<p><strong>Number of model replicas:</strong> <code>1</code></p>
</li>
<li>
<p><strong>Model server size:</strong> <code>Custom</code></p>
<div class="ulist">
<ul>
<li>
<p><strong>CPUs requested:</strong> <code>4</code></p>
</li>
<li>
<p><strong>Memory requested:</strong> <code>24 GiB</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Accelerator:</strong> <code>NVIDIA GPU</code></p>
</li>
<li>
<p><strong>Number of accelerators:</strong> <code>1</code></p>
</li>
</ul>
</div>
</div>
</div>
</li>
<li>
<p>In the <strong>Model location</strong> section, for the <strong>Source</strong>, select <code>Use a data connection</code>. Enter the following:</p>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><strong>Connection name:</strong> <code>granite-8b-instruct-oci</code></p>
</li>
<li>
<p><strong>URI:</strong> <code>oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct</code></p>
</li>
</ul>
</div>
</div>
</div>
</li>
<li>
<p>Click <strong>Deploy</strong>.</p>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>We are using <strong>ModelCar</strong> to deploy this model. ModelCar packages a model&#8217;s weights into an OCI container image, which allows it to be stored in a registry like Quay.io and managed using standard CI/CD and GitOps promotion workflows.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_2_observe_and_diagnose_the_failure"><a class="anchor" href="#_step_2_observe_and_diagnose_the_failure"></a>Step 2: Observe and Diagnose the Failure</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The deployment will begin, and a new pod will be created. As expected, this pod will fail because one GPU doesn&#8217;t have enough memory.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In your terminal, watch the pods in the <code>serving-at-scale</code> project:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pods -w -n serving-at-scale</code></pre>
</div>
</div>
<div class="paragraph">
<p>You will see a pod named <code>granite-8b-predictor-00001-deployment-&#8230;&#8203;</code> attempt to start, but it will eventually enter an <code>Error</code> or <code>CrashLoopBackOff</code> state.</p>
</div>
</li>
<li>
<p>Once the pod has failed, retrieve its logs to diagnose the issue. Use the pod name from the previous step:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc logs granite-8b-predictor-00001-deployment-&lt;your-pod-hash&gt; -n serving-at-scale</code></pre>
</div>
</div>
</li>
<li>
<p>In the logs, you will find a <code>ValueError</code> from vLLM confirming our hypothesis:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-text hljs" data-lang="text">ValueError: To serve at least one request with the models's max seq len (131072), 20.00 GiB KV cache is needed, which is larger than the available KV cache memory (3.84 GiB). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_3_redeploy_with_tensor_parallelism_2_gpus"><a class="anchor" href="#_step_3_redeploy_with_tensor_parallelism_2_gpus"></a>Step 3: Redeploy with Tensor Parallelism (2 GPUs)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now, let&#8217;s fix the deployment by providing the necessary resources.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Return to the <strong>Model Serving</strong> page in the OpenShift AI Dashboard.</p>
</li>
<li>
<p>Find the <code>granite-8b</code> deployment, click the three-dots menu on the right, and select <strong>Edit</strong>.</p>
</li>
<li>
<p>Make two critical changes:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p><strong>Number of accelerators:</strong> Change this value from <code>1</code> to <code>2</code>.</p>
</li>
<li>
<p><strong>Additional serving runtime arguments:</strong> Add the following argument: <code>--tensor-parallel-size=2</code></p>
</li>
</ol>
</div>
</li>
<li>
<p>Click <strong>Deploy</strong>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This will trigger a new deployment. OpenShift AI will create a new pod named <code>granite-8b-predictor-00002-deployment-&#8230;&#8203;</code>. This time, the deployment will succeed because vLLM can now shard the model across both GPUs.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_4_verify_gpu_utilization"><a class="anchor" href="#_step_4_verify_gpu_utilization"></a>Step 4: Verify GPU Utilization</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let&#8217;s confirm that both GPUs are being used by the model server.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Get the name of the new, running pod:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get pods -n serving-at-scale</code></pre>
</div>
</div>
</li>
<li>
<p>Use <code>rsh</code> to get a shell inside the running pod:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc rsh granite-8b-predictor-00002-deployment-&lt;your-pod-hash&gt; -n serving-at-scale</code></pre>
</div>
</div>
</li>
<li>
<p>Inside the pod&#8217;s shell, run the NVIDIA System Management Interface (<code>nvidia-smi</code>) command:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">nvidia-smi</code></pre>
</div>
</div>
<div class="paragraph">
<p>You will see output showing two NVIDIA GPUs (GPU 0 and GPU 1). In the processes table at the bottom, you should see that there are two <code>python</code> processes, one running on each GPU, with a significant amount of vRAM utilized on each. This confirms that tensor parallelism is working.</p>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_5_test_the_deployed_model_endpoint"><a class="anchor" href="#_step_5_test_the_deployed_model_endpoint"></a>Step 5: Test the Deployed Model Endpoint</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Finally, let&#8217;s ensure the model is responsive by testing its API endpoint.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the OpenShift AI Dashboard, on the <strong>Model Serving</strong> page for <code>granite-8b</code>, find the <strong>Inference endpoint</strong> URL and copy it.</p>
</li>
<li>
<p>Open a new browser tab and paste the URL, adding <code>/docs</code> to the end. This will open the FastAPI Swagger UI for your vLLM instance.</p>
</li>
<li>
<p>Find the <code>POST /v1/chat/completions</code> endpoint and expand it. Click <strong>Try it out</strong>.</p>
</li>
<li>
<p>In the <strong>Request body</strong>, paste the following JSON payload:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
  "model": "granite-8b",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful Red Hat expert."
    },
    {
      "role": "user",
      "content": "What is OpenShift AI?"
    }
  ]
}</code></pre>
</div>
</div>
</li>
<li>
<p>Click <strong>Execute</strong>. You should receive a successful JSON response from the model with a detailed answer to your question.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Congratulations! You have successfully deployed, troubleshot, and scaled an LLM across multiple GPUs.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs"></code></pre>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="vllm-adv.html">Advanced vLLM Configuration</a></span>
  <span class="next"><a href="gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
