<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Architecting GPU as a Service :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="intro.html">
    <link rel="next" href="gpu-sharing.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mps.html">NVIDIAâ€™s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section2.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section3.html">GPU Sharing Technologies</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Lab 1: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="intro.html">Serving at Scale</a></li>
    <li><a href="gpu-aas.html">Architecting GPU as a Service</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Architecting GPU as a Service</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to the foundational module of our Models-as-a-Service workshop. Before we can serve AI models, we must first build the high-performance, cost-effective infrastructure they will run on. This module provides the blueprint for transforming your GPU resources from siloed, underutilized assets into a centralized, elastic "GPU as a Service" platform on OpenShift AI.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_strategic_imperative_unlocking_gpu_value"><a class="anchor" href="#_the_strategic_imperative_unlocking_gpu_value"></a>The Strategic Imperative: Unlocking GPU Value</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Modern AI, especially Large Language Models (LLMs), depends on the immense computational power of GPUs like the NVIDIA A100 and H100. However, the traditional model of allocating dedicated GPUs per project is plagued with inefficiency, leading to inflated costs and bottlenecks that slow down innovation.</p>
</div>
<div class="paragraph">
<p>The solution is to treat GPUs not as project-specific hardware, but as a centrally managed, shared service. This paradigm shift delivers transformative value:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Significant Cost Reduction:</strong> Maximize the utilization of your existing GPU fleet (from an average of 30-40% to over 80%).</p>
</li>
<li>
<p><strong>Accelerated Access:</strong> Enable developer self-service, allowing teams to provision GPU resources in minutes, not weeks.</p>
</li>
<li>
<p><strong>Democratized Innovation:</strong> Empower more teams to experiment and build AI applications without the barrier of hardware procurement.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_core_architectural_capabilities"><a class="anchor" href="#_core_architectural_capabilities"></a>Core Architectural Capabilities</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Building this platform requires two core sets of capabilities: <strong>sharing</strong> a single GPU among many smaller workloads and <strong>aggregating</strong> many GPUs for a single, massive workload.</p>
</div>
<div class="sect2">
<h3 id="_flexible_gpu_sharing_for_concurrent_workloads"><a class="anchor" href="#_flexible_gpu_sharing_for_concurrent_workloads"></a>Flexible GPU Sharing for Concurrent Workloads</h3>
<div class="paragraph">
<p>GPU sharing techniques allow multiple workloads to run concurrently on a single physical GPU, maximizing utilization for development, testing, and smaller inference tasks.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Multi-Instance GPU (MIG)</dt>
<dd>
<p>Partitions a single physical GPU into up to seven smaller, fully isolated "virtual GPUs." Each instance has its own dedicated memory and compute, providing strict hardware-level isolation and predictable performance.</p>
</dd>
<dt class="hdlist1">GPU Timeslicing</dt>
<dd>
<p>A software-based technique where multiple workloads take turns accessing the GPU. It is highly effective but offers no memory isolation, which can lead to resource contention.</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_gpu_aggregation_for_large_scale_ai_models"><a class="anchor" href="#_gpu_aggregation_for_large_scale_ai_models"></a>GPU Aggregation for Large-Scale AI Models</h3>
<div class="paragraph">
<p>When a single model is too large to fit into one GPU&#8217;s memory, aggregation techniques (or "distributed inference") are essential. This is the key to serving today&#8217;s most powerful LLMs.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Tensor Parallelism</dt>
<dd>
<p>Shards a model&#8217;s layers across multiple GPUs, typically within a single node connected by a high-bandwidth interconnect like NVLink. This reduces latency by processing parts of the model concurrently.</p>
</dd>
<dt class="hdlist1">Pipeline Parallelism</dt>
<dd>
<p>Partitions the model into a sequence of stages, with each stage running on a different GPU or node. This is crucial for models so large they exceed the capacity of all GPUs within a single machine.</p>
</dd>
<dt class="hdlist1">Data Parallelism</dt>
<dd>
<p>Replicates the same model across multiple GPUs and distributes data batches among them. While primarily a training technique, it can be useful for isolating user request batches during inference.</p>
</dd>
<dt class="hdlist1">Expert Parallelism</dt>
<dd>
<p>A specialized technique for Mixture of Experts (MoE) models that assigns different "expert" sub-networks to dedicated GPUs, leveraging sparse activation patterns to scale efficiently.</p>
</dd>
</dl>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_putting_theory_into_practice_distributed_inference_with_vllm"><a class="anchor" href="#_putting_theory_into_practice_distributed_inference_with_vllm"></a>Putting Theory into Practice: Distributed Inference with vLLM</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While these concepts apply broadly, our focus is on implementing them with vLLM, a high-performance inference engine. Serving large models with vLLM often leads to memory bottlenecks, such as the dreaded <code>CUDA out of memory</code> error. Distributed inference is the primary solution.</p>
</div>
<div class="sect2">
<h3 id="_choosing_your_vllm_distribution_strategy"><a class="anchor" href="#_choosing_your_vllm_distribution_strategy"></a>Choosing Your vLLM Distribution Strategy</h3>
<div class="paragraph">
<p>The decision of which strategy to use depends on your model size, hardware, and performance goals.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>Single GPU (No Distributed Inference)</strong></dt>
<dd>
<p>If your model fits comfortably in a single GPU&#8217;s memory, this is the simplest option with the least overhead.</p>
</dd>
<dt class="hdlist1"><strong>Single-Node, Multi-GPU (Tensor Parallelism)</strong></dt>
<dd>
<p>When your model exceeds single GPU capacity but fits within one node, use tensor parallelism. Set the <code>tensor_parallel_size</code> to the number of available GPUs on the node.</p>
<div class="listingblock">
<div class="title">Example Configuration: 4 GPUs in a single node</div>
<div class="content">
<pre>tensor_parallel_size = 4</pre>
</div>
</div>
</dd>
<dt class="hdlist1"><strong>Multi-Node, Multi-GPU (Tensor + Pipeline Parallelism)</strong></dt>
<dd>
<p>For models that exceed single-node capacity, combine tensor and pipeline parallelism. <code>tensor_parallel_size</code> is the number of GPUs <strong>per node</strong>, and <code>pipeline_parallel_size</code> is the number of nodes.</p>
<div class="listingblock">
<div class="title">Example Configuration: 16 GPUs across 2 nodes (8 GPUs per node)</div>
<div class="content">
<pre>tensor_parallel_size = 8
pipeline_parallel_size = 2</pre>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_the_as_a_service_enablement_layer"><a class="anchor" href="#_the_as_a_service_enablement_layer"></a>The "As a Service" Enablement Layer</h3>
<div class="paragraph">
<p>Finally, to deliver a true self-service experience, we need an enablement layer on top of our GPU infrastructure. This includes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>API Gateway (e.g., Red Hat 3scale):</strong> Provides a unified, secure "front door" for developers.</p>
</li>
<li>
<p><strong>Identity and Access Management (IAM):</strong> Authenticates users and authorizes access.</p>
</li>
<li>
<p><strong>Observability:</strong> Delivers critical logging and metrics for monitoring, cost allocation, and capacity planning.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_useful_links"><a class="anchor" href="#_useful_links"></a>Useful Links</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Distributed Inference with vLLM - GPU Parallelism Techniques</p>
<div class="paragraph">
<p><a href="https://developers.redhat.com/articles/2025/02/06/distributed-inference-with-vllm#gpu_parallelism_techniques_in_vllm" class="bare" target="_blank" rel="noopener">https://developers.redhat.com/articles/2025/02/06/distributed-inference-with-vllm#gpu_parallelism_techniques_in_vllm</a></p>
</div>
</li>
<li>
<p>How We Optimized vLLM DeepSeek R1 - Open Infra Week Contributions</p>
<div class="paragraph">
<p><a href="https://developers.redhat.com/articles/2025/03/19/how-we-optimized-vllm-deepseek-r1#open_infra_week_contributions" class="bare" target="_blank" rel="noopener">https://developers.redhat.com/articles/2025/03/19/how-we-optimized-vllm-deepseek-r1#open_infra_week_contributions</a></p>
</div>
</li>
<li>
<p>GPU Partitioning Guide</p>
<div class="paragraph">
<p><a href="https://github.com/rh-aiservices-bu/gpu-partitioning-guide" class="bare" target="_blank" rel="noopener">https://github.com/rh-aiservices-bu/gpu-partitioning-guide</a></p>
</div>
</li>
</ol>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="intro.html">Serving at Scale</a></span>
  <span class="next"><a href="gpu-sharing.html">GPU Sharing Technologies</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
