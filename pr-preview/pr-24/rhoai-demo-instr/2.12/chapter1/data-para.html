<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Data Parallelism :: OpenShift AI Lab environment instructions</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/rhaiis_overview.html">Red Hat AI Inference Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/gpu-rhaiis.html">GPUs for AI Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="data-para.html">Data Parallelism</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Data Parallelism</h1>
<div class="sect1">
<h2 id="_the_new_problem_your_service_is_too_popular"><a class="anchor" href="#_the_new_problem_your_service_is_too_popular"></a>The New Problem: Your Service is Too Popular</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You have successfully deployed your large model using a combination of Tensor and Pipeline Parallelism. The model is running, but now you face a new challenge: demand is overwhelming your service. You have hundreds of concurrent users, requests are queuing up, and overall throughput is too low to meet the business need.</p>
</div>
<div class="paragraph">
<p>Your bottleneck is no longer model size; it&#8217;s <strong>serving capacity</strong>. This is the problem that <strong>Data Parallelism</strong> is designed to solve.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_how_it_works_the_bank_teller_analogy"><a class="anchor" href="#_how_it_works_the_bank_teller_analogy"></a>How It Works: The "Bank Teller" Analogy</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Data Parallelism is the most intuitive scaling strategy. It works by creating multiple, independent copies (replicas) of your model and distributing user requests among them.</p>
</div>
<div class="paragraph">
<p>Think of a busy bank:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>One Teller (A Single Model Instance):</strong> A single bank teller can only serve one customer at a time. As the line of customers grows, the wait time increases dramatically.</p>
</li>
<li>
<p><strong>Opening More Teller Windows (Data Parallelism):</strong> To handle the long line, the bank manager opens more teller windows. Each new teller is a complete, independent copy of the service. Now, multiple customers can be served simultaneously, drastically increasing the bank&#8217;s overall throughput.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This is exactly how Data Parallelism works. Each "teller window" is a full replica of your AI model running on its own dedicated GPU(s).</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/ddp.gif" alt="Distributed Data Parallelism" width="600">
</div>
<div class="title">Figure 1. A visual example of Data Parallelism</div>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_how_data_parallelism_applies_to_inference"><a class="anchor" href="#_how_data_parallelism_applies_to_inference"></a>How Data Parallelism Applies to Inference</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While Data Parallelism is a cornerstone of distributed <strong>training</strong> (where it&#8217;s used to process huge datasets faster and synchronize model gradients), its application in <strong>inference</strong> is simpler and focused purely on throughput.</p>
</div>
<div class="paragraph">
<p>In a MaaS inference context:
1.  <strong>Replication:</strong> The entire model is replicated onto multiple GPUs. Each GPU (or set of GPUs, if using Tensor/Pipeline Parallelism) runs an independent copy.
2.  <strong>Request Distribution:</strong> A load balancer, either within the vLLM engine itself or at the API Gateway level, receives all incoming user requests.
3.  <strong>Dispatching:</strong> The load balancer dispatches each new request to the next available model replica.
4.  <strong>Independent Processing:</strong> Each replica processes its assigned request independently. There is no cross-communication or synchronization of results between the replicas.</p>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
