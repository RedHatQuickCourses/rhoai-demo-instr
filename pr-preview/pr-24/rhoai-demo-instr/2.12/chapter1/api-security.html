<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Securing the AI Factory: A Defense-in-Depth Approach :: OpenShift AI Lab environment instructions</title>
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="2.12">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OpenShift AI Lab environment instructions</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/rhaiis_overview.html">Red Hat AI Inference Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/gpu-rhaiis.html">GPUs for AI Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OpenShift AI Lab environment instructions</span>
    <span class="version">2.12</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OpenShift AI Lab environment instructions</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">2.12</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OpenShift AI Lab environment instructions</a></li>
    <li><a href="api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Securing the AI Factory: A Defense-in-Depth Approach</h1>
<div class="sect1">
<h2 id="_why_ai_security_is_different"><a class="anchor" href="#_why_ai_security_is_different"></a>Why AI Security is Different</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Securing a Models-as-a-Service platform requires more than just standard application security. AI and Large Language Models introduce new, unique attack surfaces that traditional security measures are not designed to handle.</p>
</div>
<div class="paragraph">
<p>As a consultant, you must be able to articulate these new risks to customers:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Model Poisoning:</strong> Malicious actors can tamper with training data or the model file itself to introduce hidden backdoors or biases.</p>
</li>
<li>
<p><strong>Prompt Injection:</strong> Attackers can craft malicious inputs (prompts) to bypass safety guardrails, reveal confidential information from the model&#8217;s context, or cause the model to execute unintended actions.</p>
</li>
<li>
<p><strong>Data Extraction Attacks:</strong> Adversaries can interact with a model to try and reconstruct the sensitive, proprietary data it was trained on.</p>
</li>
<li>
<p><strong>Denial of Service (Resource Exhaustion):</strong> AI inference is resource-intensive. An attacker can easily overwhelm a service with a few complex requests, leading to massive costs and service outages.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To protect against these threats, we must adopt a <strong>Defense-in-Depth</strong> strategy, building multiple layers of security throughout our AI Factory.</p>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_the_layers_of_defense_for_the_ai_factory"><a class="anchor" href="#_the_layers_of_defense_for_the_ai_factory"></a>The Layers of Defense for the AI Factory</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A robust security posture is built in layers, from the code and models themselves to the infrastructure they run on and the policies that govern them.</p>
</div>
<div class="sect2">
<h3 id="_layer_1_securing_the_ai_supply_chain"><a class="anchor" href="#_layer_1_securing_the_ai_supply_chain"></a>Layer 1: Securing the AI Supply Chain</h3>
<div class="paragraph">
<p>Security starts before the model is ever deployed. We must ensure that the assets we are building with—models, data, and code—are trusted and free from tampering.</p>
</div>
<div class="ulist">
<div class="title">Checklist</div>
<ul>
<li>
<p><strong>Sign and Verify Model Artifacts:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Model Poisoning.
<em>Control:</em> Use tools like <code>sigstore/cosign</code> to cryptographically sign model files. The platform should then verify this signature before allowing a model to be deployed, ensuring it hasn&#8217;t been altered.</p>
</div>
</div>
</div>
</li>
<li>
<p><strong>Use a Trusted Model Registry:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Using compromised or untrusted models.
<em>Control:</em> Store curated, approved, and scanned models in a secure, private registry, just as you do for container images.</p>
</div>
</div>
</div>
</li>
<li>
<p><strong>Generate and Maintain an AI Bill of Materials (AIBOM):</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Lack of transparency and provenance.
<em>Control:</em> Maintain a record of the model&#8217;s lineage: what data it was trained on, its core architecture, and its known limitations. This is crucial for accountability and troubleshooting.</p>
</div>
</div>
</div>
</li>
<li>
<p><strong>Scan All Dependencies (SBOM):</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Vulnerabilities in third-party code.
<em>Control:</em> Use tools like Syft and Grype to generate a Software Bill of Materials (SBOM) and scan all dependencies in your inference server code for known CVEs.</p>
</div>
</div>
</div>
</li>
</ul>
</div>
<hr>
</div>
<div class="sect2">
<h3 id="_layer_2_securing_the_platform_infrastructure"><a class="anchor" href="#_layer_2_securing_the_platform_infrastructure"></a>Layer 2: Securing the Platform &amp; Infrastructure</h3>
<div class="paragraph">
<p>This layer focuses on securing the underlying OpenShift AI environment where the models will run.</p>
</div>
<div class="ulist">
<div class="title">Checklist</div>
<ul>
<li>
<p><strong>Run Models in Isolated, Hardened Runtimes:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Container breakouts and privilege escalation.
<em>Control:</em> Leverage OpenShift&#8217;s built-in security by running model containers as non-root users with minimal privileges, enforced by security contexts and technologies like SELinux.</p>
</div>
</div>
</div>
</li>
<li>
<p><strong>Encrypt Data At Rest and In Transit:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Data interception or theft from storage.
<em>Control:</em> Enforce TLS for all communication. Use OpenShift&#8217;s storage features with KMS integration to ensure all data, including model weights and cached datasets, is encrypted on disk.</p>
</div>
</div>
</div>
</li>
<li>
<p><strong>Apply Policy-as-Code with OpenShift GitOps:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Inconsistent or insecure deployments.
<em>Control:</em> Use GitOps to declaratively manage all MaaS configurations. This ensures that every deployment adheres to a version-controlled, peer-reviewed security standard.</p>
</div>
</div>
</div>
</li>
<li>
<p><strong>Utilize Advanced Cluster Security (ACS):</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Runtime vulnerabilities and misconfigurations.
<em>Control:</em> Implement Red Hat Advanced Cluster Security to scan running containers, enforce network policies, and detect anomalous behavior within the cluster.</p>
</div>
</div>
</div>
</li>
</ul>
</div>
<hr>
</div>
<div class="sect2">
<h3 id="_layer_3_securing_the_service_api_endpoint"><a class="anchor" href="#_layer_3_securing_the_service_api_endpoint"></a>Layer 3: Securing the Service &amp; API Endpoint</h3>
<div class="paragraph">
<p>This layer protects the "front door" of your AI Factory, controlling who can access the service and what they can do.</p>
</div>
<div class="ulist">
<div class="title">Checklist</div>
<ul>
<li>
<p><strong>Enforce Strong Authentication and Authorization:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Unauthorized access.
<em>Control:</em> Secure all model endpoints through an API Gateway integrated with a corporate SSO (using OIDC/OAuth2). Every request must have a validated user token and application API key.</p>
</div>
</div>
</div>
</li>
<li>
<p><strong>Validate All Input Payloads:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Prompt Injection and Denial of Service.
<em>Control:</em> The API Gateway or the inference service itself should validate all incoming data, checking for malicious patterns, and enforcing size limits to prevent resource exhaustion attacks.</p>
</div>
</div>
</div>
</li>
<li>
<p><strong>Implement Comprehensive Auditing and Monitoring:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Undetected malicious activity.
<em>Control:</em> Log every API request and model invocation. Integrate these logs with a SIEM (like Splunk) and monitor resource usage in Prometheus to detect anomalies, such as a sudden spike in token usage from a single user.</p>
</div>
</div>
</div>
</li>
</ul>
</div>
<hr>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_layer_4_governance_risk_and_compliance_grc"><a class="anchor" href="#_layer_4_governance_risk_and_compliance_grc"></a>Layer 4: Governance, Risk, and Compliance (GRC)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This final layer is about the human processes and policies that govern the entire platform.</p>
</div>
<div class="ulist">
<div class="title">Checklist</div>
<ul>
<li>
<p><strong>Conduct AI Risk Assessments:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Unforeseen business or ethical risks.
<em>Control:</em> Regularly assess the AI models and platform against a standard framework like the NIST AI Risk Management Framework (RMF) or ISO 42001.</p>
</div>
</div>
</div>
</li>
<li>
<p><strong>Ensure Regulatory Compliance:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Legal and financial penalties.
<em>Control:</em> If the platform will handle sensitive information, ensure all security controls meet the requirements of relevant regulations like GDPR, HIPAA, etc.</p>
</div>
</div>
</div>
</li>
<li>
<p><strong>Maintain Incident Response Plans:</strong></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Threat:</em> Being unprepared for a security event.
<em>Control:</em> Develop and document a plan for how to respond to AI-specific incidents, such as a model leak, a major data extraction event, or the discovery of a critical bias.</p>
</div>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
