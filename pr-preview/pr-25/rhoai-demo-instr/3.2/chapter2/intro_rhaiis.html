<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Model Serving with Red Hat AI Inference Server :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="../chapter5/section4.html">
    <link rel="next" href="rhaiis_overview.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="3.2">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/gpu-mps.html">NVIDIA’s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../chapter1/expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../chapter1/api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhaiis_overview.html"><strong>Red Hat AI Inference Server (RHAIIS)</strong></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="gpu-rhaiis.html">GPUs for AI Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Model Serving with Red Hat AI Inference Server (RHAIIS)</span>
    <span class="version">3.2</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">3.2</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a></li>
    <li><a href="intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Model Serving with Red Hat AI Inference Server</h1>
<div class="sect1">
<h2 id="_overview_and_objectives"><a class="anchor" href="#_overview_and_objectives"></a>Overview and Objectives</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Welcome to Model Serving with Red Hat AI Inference Server (RHAIIS).
This is a hands-on lab designed to give you practical experience serving large language models (LLMs) with Red Hat AI Inference Server (RHAIIS).</p>
</div>
<div class="paragraph">
<p>By the end of this course, you will be able to:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Deploy and serve an LLM for inference using two different Granite models (2B and 8B parameters), gaining firsthand experience with a powerful, enterprise-grade platform.</p>
</li>
<li>
<p>Optimize GPU resource usage by monitoring memory consumption in real time, understanding how RHAIIS loads model weights and manages the KV Cache. You&#8217;ll learn how to fine-tune performance by controlling key parameters like max_tokens.</p>
</li>
<li>
<p>Troubleshoot and solve deployment challenges, specifically by working through the advanced steps needed to successfully launch a larger 8B model. This will build your skills for real-world scenarios.</p>
</li>
<li>
<p>Lay a foundation for further exploration by using the Red Hat AI Model Repository on Hugging Face to serve and experiment with more models after the lab exercises.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>While the lab environment initializes, take this opportunity to review the provided lab guide. It covers essential topics like RHAIIS requirements, supported deployments, and advanced vLLM configuration settings, giving you the context you need to succeed.</p>
</div>
<div class="paragraph">
<p>Ready to get started? Let’s dive into a powerful platform that helps you deploy AI models with flexibility and high performance across any hybrid cloud environment.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_outcomes"><a class="anchor" href="#_outcomes"></a>Outcomes</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Upon completing this lab, you will be able to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Deploy AI Inference server for Huggingface based models using Podman.</p>
</li>
<li>
<p>Verify the model is serving correctly by interacting with its API.</p>
</li>
<li>
<p>Monitor the GPU&#8217;s video memory (VRAM) usage in real-time.</p>
</li>
<li>
<p>Tune server parameters to control memory consumption and context length.</p>
</li>
<li>
<p>Deploy and test an alternative model to see the platform&#8217;s flexibility.</p>
</li>
<li>
<p>Determine the max-model-len for a given model.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_environment_prerequisites"><a class="anchor" href="#_environment_prerequisites"></a>Environment Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Your lab environment has been pre-configured with the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A Red Hat Enterprise Linux 9.x system with a valid subscription.</p>
</li>
<li>
<p>An attached and configured NVIDIA data center GPU with drivers installed.</p>
</li>
<li>
<p>Podman and the NVIDIA Container Toolkit are pre-installed.</p>
</li>
<li>
<p>Credentials for <strong>Red Hat account</strong> to access <code>registry.redhat.io</code>. (Provided for this experience)</p>
</li>
<li>
<p>A <strong>Hugging Face account</strong> with a User Access Token with read permissions. (Provided for this experience)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_setting_up_your_rhaiis_lab_environment"><a class="anchor" href="#_setting_up_your_rhaiis_lab_environment"></a>Setting Up Your RHAIIS Lab Environment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A dedicated lab environment for this training is currently in development. In the meantime, you can use the same environment available on the Demo Platform, <a href="https://catalog.demo.redhat.com/catalog?item=babylon-catalog-prod/rhdp.rhaiis-on-rhel.prod&amp;utm_source=webapp&amp;utm_medium=share-link" target="blank"><strong>Base Red Hat AI Inference Server (RHAIIS)</strong></a> - which is pre-configured with the Red Hat AI Inference Server (RHAIIS).</p>
</div>
<div class="paragraph">
<p>This environment includes some pre-configured bonus content:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A bonus lab that shows you how to connect to the AI model using Python.</p>
</li>
<li>
<p>An Qwen2.5 model running on RHAIIS as a system service. This model starts automatically when the system boots.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Once you have completed the initial exercises, you can stop this service to free up the environment for the this lab&#8217;s main activities. To stop the service, simply run the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo systemctl stop rhaiis.service</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_interactive_experiences_arcades"><a class="anchor" href="#_interactive_experiences_arcades"></a>Interactive Experiences - Arcades</h2>
<div class="sectionbody">
<div class="paragraph">
<p>These embedded interactive experiences are woven throughout the training materials. At times they can malfunction when run from a small window, they work best in full screen.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>There is a navigation bar across the top.</p>
<div class="ulist">
<ul>
<li>
<p>use the diagonal arrows at the top right to expand the window.</p>
</li>
<li>
<p>use the arrows at the top left to navigate forward or backwards.</p>
</li>
<li>
<p>in the center of the screen you can restart the Arcade.</p>
</li>
<li>
<p>the lower section of the screen show video progress or slide progression steps.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<iframe
  src="https://demo.arcade.software/isTBAIOGdpdy2RDLO9ya?embed&embed_mobile=inline&embed_desktop=inline&show_copy_link=true"
  width="100%"
  height="600px"
  frameborder="0"
  allowfullscreen
  webkitallowfullscreen
  mozallowfullscreen
  allow="clipboard-write"
  muted>
</iframe>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="../chapter5/section4.html">blank</a></span>
  <span class="next"><a href="rhaiis_overview.html"><strong>Red Hat AI Inference Server (RHAIIS)</strong></a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
