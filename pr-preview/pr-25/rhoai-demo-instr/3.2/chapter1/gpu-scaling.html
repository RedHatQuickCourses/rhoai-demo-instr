<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Scaling Architectures :: OpenShift AI Lab environment instructions</title>
    <link rel="prev" href="gpu-sizing.html">
    <link rel="next" href="vllm-adv.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">OpenShift AI Lab environment instructions</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai-demo-instr" data-version="3.2">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Why Customers Need a MaaS Platform</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="intro.html">Serving at Scale</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-aas.html">Architecting GPU as a Service</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-sharing.html">GPU Sharing Technologies</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-time.html">GPU Timeslicing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mig.html">Multi-Instance GPU (MIG)</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="gpu-mps.html">NVIDIAâ€™s Multi-Process Service (MPS)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="gpu-agg.html">GPU Aggregation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="tensor.html">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="pipeline.html">Pipeline Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="data-para.html">Data Parallelism</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="expert-para.html">Expert Parallelism</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="start.html">Models with GitOps</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-sizing.html">Sizing for GPUs</a>
  </li>
  <li class="nav-item is-current-page" data-depth="3">
    <a class="nav-link" href="gpu-scaling.html">Scaling Architectures</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-multi.html">Lab: Multi-GPU Model Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gpu-nodes.html">Lab: Multi-Node, Multi-GPU Deployment</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="gitops.html">Deploying Models with GitOps</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="api-reqs.html">API Gateway Requirements for MaaS</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="index.html">MaaS Logical Architecture</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="intro2.html">The Gateway&#8217;s Core Requirements</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api_checklist.html">Hands-On Lab: Productizing and Consuming an AI Model</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="api-security.html">Securing the AI Factory: A Defense-in-Depth Approach</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter6/index.html">Agentic AI Placeholder</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter6/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter5/index.html">LLM optimization and inference efficiency</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section1.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section2.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section3.html">blank</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter5/section4.html">blank</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/intro_rhaiis.html">Model Serving with Red Hat AI Inference Server</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/rhaiis_overview.html"><strong>Red Hat AI Inference Server (RHAIIS)</strong></a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/gpu-rhaiis.html">GPUs for AI Inference</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/vllm-adv.html">Advanced vLLM Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/rhaiis_lab.html">Lab: Model Serving with Red Hat AI Inference Server</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Model Serving with Red Hat AI Inference Server (RHAIIS)</span>
    <span class="version">3.2</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">3.2</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Model Serving with Red Hat AI Inference Server (RHAIIS)</a></li>
    <li><a href="intro.html">Serving at Scale</a></li>
    <li><a href="start.html">Models with GitOps</a></li>
    <li><a href="gpu-scaling.html">Scaling Architectures</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Scaling Architectures</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As you begin to deploy larger and more complex Large Language Models, you will quickly encounter the physical limits of a single GPU. To overcome this, the vLLM serving runtime supports two primary scaling architectures: scaling <strong>up</strong> within a single node and scaling <strong>out</strong> across multiple nodes.</p>
</div>
<div class="paragraph">
<p>Understanding the difference between these two patterns, their underlying technologies, and their performance trade-offs is crucial for designing a cost-effective and efficient AI serving platform. This section provides a conceptual overview of these deployment strategies before you implement them in the hands-on labs.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_single_node_multi_gpu"><a class="anchor" href="#_single_node_multi_gpu"></a>Single-Node, Multi-GPU</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This is the most common and performant approach for scaling up. When a model is too large for one GPU, you can deploy it on a single OpenShift node that has multiple GPUs available.</p>
</div>
<div class="paragraph">
<p>vLLM uses <strong>Tensor Parallelism</strong> to automatically shard the model&#8217;s weights across all the available GPUs within the node. The <code>--tensor-parallel-size</code> argument is used to tell vLLM how many GPUs to use. This creates a larger, unified pool of vRAM to accommodate the model and its KV Cache.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/03-single-node-multi-gpu.png" alt="A diagram showing a model sharded across multiple GPUs within a single node">
</div>
<div class="title">Figure 1. Single-Node, Multi-GPU Architecture</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_multi_node_multi_gpu"><a class="anchor" href="#_multi_node_multi_gpu"></a>Multi-Node, Multi-GPU</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For the most massive models that exceed the capacity of even a fully-equipped single node, you must scale out across multiple nodes. OpenShift AI provides a multi-node vLLM distribution that uses a <strong>Ray</strong> backend to orchestrate this complex deployment.</p>
</div>
<div class="paragraph">
<p>This architecture uses a combination of two parallelism techniques:
* <strong>Pipeline Parallelism</strong>: The model&#8217;s layers are split across different nodes. The <code>--pipeline-parallel-size</code> argument typically corresponds to the number of nodes in the cluster.
* <strong>Tensor Parallelism</strong>: Within each node, the model&#8217;s layers are further sharded across all available GPUs using <code>--tensor-parallel-size</code>.</p>
</div>
<div class="paragraph">
<p>A "head" pod manages the Ray cluster, while "worker" pods execute their portion of the model.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="_images/03-multi-node-multi-gpu.png" alt="A diagram showing a model distributed across multiple nodes" width="with each node using multiple GPUs">
</div>
<div class="title">Figure 2. Multi-Node, Multi-GPU Architecture</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The Ray instance used for multi-node vLLM is embedded and managed automatically by the vLLM runtime. It does <strong>not</strong> depend on any external Ray cluster or other distributed computing tools like KubeRay or CodeFlare.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_performance_and_sharding_considerations"><a class="anchor" href="#_performance_and_sharding_considerations"></a>Performance and Sharding Considerations</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_performance_limitations_of_multi_node"><a class="anchor" href="#_performance_limitations_of_multi_node"></a>Performance Limitations of Multi-Node</h3>
<div class="paragraph">
<p>While powerful, multi-node deployments introduce network latency. The communication between nodes is significantly slower than the high-speed NVLink interconnects between GPUs within a single node.</p>
</div>
<div class="paragraph">
<p>Therefore, a model deployed on a single node with 8 GPUs will almost always have better performance (lower latency) than the same model deployed on two nodes with 4 GPUs each. <strong>It is always recommended to scale up within a single node whenever possible.</strong></p>
</div>
</div>
<div class="sect2">
<h3 id="_model_sharding_constraints"><a class="anchor" href="#_model_sharding_constraints"></a>Model Sharding Constraints</h3>
<div class="paragraph">
<p>Models cannot be sharded across an arbitrary number of GPUs. The model&#8217;s architecture, specifically the <code>num_attention_heads</code> parameter found in its <code>config.json</code> file, must be divisible by the total number of GPUs being used for tensor parallelism.</p>
</div>
<div class="paragraph">
<p>For example, the <code>ibm-granite/granite-3.3-8b-instruct</code> model has 32 attention heads. This means it can be sharded across 2, 4, or 8 GPUs, but it <strong>cannot</strong> be deployed with 3 GPUs, as 32 is not divisible by 3.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="gpu-sizing.html">Sizing for GPUs</a></span>
  <span class="next"><a href="vllm-adv.html">Advanced vLLM Configuration</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
